\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\nicematrix@redefine@check@rerun 
\citation{10.1145/1968.1972}
\citation{Vapnik1999-VAPTNO}
\citation{6797087}
\citation{belkin_reconciling_2019}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{Vapnik1999-VAPTNO,10.5555/2371238,10.5555/2621980,STL_Hajek_Maxim_2021,bousquet2020theoryuniversallearning}
\citation{6797087,Domingos2000AUB}
\citation{belkin_reconciling_2019,schaeffer_double_2023,nakkiran_deep_2019,lafon_understanding_2024}
\citation{davies_unifying_2023,d_ascoli_triple_2020}
\citation{belkin_reconciling_2019}
\citation{nakkiran_deep_2019}
\citation{lafon_understanding_2024}
\citation{schaeffer_double_2023}
\citation{liu2023understandingroleoptimizationdouble}
\citation{davies_unifying_2023}
\citation{olmin2024understandingepochwisedoubledescent}
\@writefile{toc}{\contentsline {section}{\numberline {2}Contribution}{2}{section.2}\protected@file@percent }
\citation{sharma_bias-variance_2014,schaeffer_double_2023,nakkiran_deep_2019,belkin_reconciling_2019,6797087,unified_bias_composition,Scott_Fortmann_Bias,neal2019biasvariancetradeofftextbooksneed}
\citation{Cristianini2000AnIT}
\citation{goodfellow2016deep}
\citation{GRP_Hamilton,Scar04,lopushanskyy2024graphneuralnetworksgraph,tanis2024introductiongraphneuralnetworks,bronstein2021geometricdeeplearninggrids,Veli_kovi__2023}
\citation{Oono2020Graph}
\citation{shi2024homophilymodulatesdoubledescent}
\citation{Oono2020Graph}
\citation{belkin_reconciling_2019}
\citation{nakkiran_deep_2019}
\citation{lafon_understanding_2024}
\citation{schaeffer_double_2023}
\citation{liu2023understandingroleoptimizationdouble}
\citation{davies_unifying_2023}
\citation{olmin2024understandingepochwisedoubledescent}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related works}{3}{section.3}\protected@file@percent }
\citation{bronstein2021geometricdeeplearninggrids,Bronstein_2017}
\citation{Scar04}
\citation{GRP_Hamilton}
\citation{Sterkenburg_2024,Vapnik1999-VAPTNO,STL_Hajek_Maxim_2021}
\@writefile{toc}{\contentsline {section}{\numberline {4}Theoretical Preliminary}{4}{section.4}\protected@file@percent }
\@writefile{loe}{\contentsline {assumption}{\ifthmt@listswap Assumption~1\else \numberline {1}Assumption\fi }{4}{assumption.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Illustrative dynamic of the learning problem.} For an incremental hypothesis space sequence $\mathcal  {H}_{1},\mathcal  {H}_{2},\mathcal  {H}_{3}$, we aim to obtain the procedure $\mathcal  {T}$ that would either reach the \textit  {generalization solution} $\bm  {h}$, or the \textit  {empirical solution} $h^{*}$ for a particular hypothesis class, with respect to the concept $c$ and its observed concept $c'$. Model selection hence dictates how an algorithm or procedure than choose the best possible hypothesis to approximate the generalization solution from the empirical solution set. Do notice that here we explicitly state that the data would create a \textbf  {proxy concept} that overlaps with the concept class and of some arbitrary `distance' from the true concept by $d(c,c')$. In case the two classes overlap, there still exists the arbitrary distance. Also, we can also observe the intuitive notion of increasing hypothesis class - while it indeed can help in getting closer to the concept class, the somewhat intrinsic property of current learning theory being the relative random procedure class make it probabilistically unstable.}}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:statlearnclassical}{{1}{5}{\textbf {Illustrative dynamic of the learning problem.} For an incremental hypothesis space sequence $\mathcal {H}_{1},\mathcal {H}_{2},\mathcal {H}_{3}$, we aim to obtain the procedure $\mathcal {T}$ that would either reach the \textit {generalization solution} $\bm {h}$, or the \textit {empirical solution} $h^{*}$ for a particular hypothesis class, with respect to the concept $c$ and its observed concept $c'$. Model selection hence dictates how an algorithm or procedure than choose the best possible hypothesis to approximate the generalization solution from the empirical solution set. Do notice that here we explicitly state that the data would create a \textbf {proxy concept} that overlaps with the concept class and of some arbitrary `distance' from the true concept by $d(c,c')$. In case the two classes overlap, there still exists the arbitrary distance. Also, we can also observe the intuitive notion of increasing hypothesis class - while it indeed can help in getting closer to the concept class, the somewhat intrinsic property of current learning theory being the relative random procedure class make it probabilistically unstable}{figure.caption.1}{}}
\newlabel{eq:lp1}{{3}{6}{Empirical learning problem}{equation.3}{}}
\citation{achlioptas_stochastic_nodate,ruder_overview_2017}
\citation{zhang_gradient_2019}
\@writefile{toc}{\contentsline {section}{\numberline {5}Practical Preliminary}{7}{section.5}\protected@file@percent }
\newlabel{SC@1}{{\caption@xref {??}{ on input line 253}}{8}{}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Partitioning process and its error potential consideration.} We assume each partition includes the irreducible error $\epsilon $ accompanied by the $n$ partition, belongs to the furthest partitioning set. Within every increasing partition, for supposed distributed data (unordered data), the generalization risk is further decomposed.}}{8}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Datasets and its potential effects}{9}{subsection.5.1}\protected@file@percent }
\citation{gareth_james_introduction_2013}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Illustration of dataset optimization path, dataset $\mathcal  {S}_{1}$}. From here, we can see the absolute path (in red), the `step-size' path (in black), and the relative step distance between where the model land and the data's supposed suggested shape of $c$. We assume those `snapshot' created by data are within varying noise, however still fall in range of relatively same concept region class. As for the path, we can see that the chosen step size learning is much more stable overall, comparing to the overfitting-like path.}}{10}{figure.caption.3}\protected@file@percent }
\newlabel{fig:randomwalk_descent}{{3}{10}{\textbf {Illustration of dataset optimization path, dataset $\mathcal {S}_{1}$}. From here, we can see the absolute path (in red), the `step-size' path (in black), and the relative step distance between where the model land and the data's supposed suggested shape of $c$. We assume those `snapshot' created by data are within varying noise, however still fall in range of relatively same concept region class. As for the path, we can see that the chosen step size learning is much more stable overall, comparing to the overfitting-like path}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Overfitting and underfitting}{10}{subsection.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Illustration of dataset optimization path, dataset $\mathcal  {S}_{2}$}. Similar to $\mathcal  {S}_1$, however, the intrinsic shape and configuration of the dataset makes a further thresholding limit on the approach to $c$, hence specifically under this dataset, we suppose that any sub-partition of $S_{2}$ would not reach certain accuracy measure.}}{11}{figure.caption.4}\protected@file@percent }
\citation{LehmannCasella_theory_1998,liam_statistics_2005}
\citation{liam_statistics_2005}
\citation{piera_sample_2005,MkayPretenceSignalStatistics1993}
\citation{MkayPretenceSignalStatistics1993}
\citation{6797087}
\citation{6797087}
\@writefile{toc}{\contentsline {section}{\numberline {6}Bias-variance tradeoff}{13}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Statistical origin}{13}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Precursor (Geman et al., 1992)}{13}{subsection.6.2}\protected@file@percent }
\citation{brown2024biasvariance}
\citation{adlam2020understandingdoubledescentrequires}
\citation{brown2024biasvariance,PfauBregmanDivergence}
\citation{lafon_understanding_2024}
\citation{domingos_unifeid_2000}
\newlabel{SC@2}{{\caption@xref {??}{ on input line 350}}{15}{}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Decomposition of the error term into 3 parts}. Respectively, the irreducible error $\mathbb  {V}[y(\mathbf  {x})]$, the variance (blue) and the bias (dark yellow). All of them are assumed to take up 100\% of the error observed in such composition. The proportion is included using the three coefficient $\lambda _{\epsilon },\lambda _{V},\lambda _{B}$ where $\lambda _{\epsilon }+\lambda _{V}+\lambda _{B}=1$.}}{15}{figure.caption.5}\protected@file@percent }
\citation{6797087,sharma_bias-variance_2014,domingos_unifeid_2000,adlam2020understandingdoubledescentrequires,yang_rethinking_2020}
\citation{brown2024biasvariance}
\citation{10.5555/2371238}
\citation{10.5555/2371238,lafon_understanding_2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Statistical learning theory perspective}{16}{subsection.6.3}\protected@file@percent }
\citation{adlam2020understandingdoubledescentrequires}
\citation{hastie2019surprises,mei2019generalization}
\citation{neal2018modern}
\citation{adlam2020understandingdoubledescentrequires}
\citation{hastie2019surprises,mei2019generalization}
\citation{neal2018modern}
\citation{adlam2020understandingdoubledescentrequires}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Gradient descent on bias-variance decomposition}{17}{subsection.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces (\textbf  {a-e) The different bias-variance decompositions.} (f-j) Corresponding theoretical predictions for $\gamma =0$, $\phi =1/16$ and $\sigma = \tanh $ with $\text  {SNR} = 100$ as the model capacity varies across the interpolation threshold (dashed red). (a,f) The semi-classical decomposition of~\cite  {hastie2019surprises,mei2019generalization} has a nonmonotonic and divergent bias term, conflicting with standard definitions of the bias. (b,g) The decomposition of~\cite  {neal2018modern} utilizing the law of total variance interprets the diverging term $V_D^\textsc  {c}$ as ``variance due to optimization''. (c,h) An alternative application of the law of total variance suggests the opposite, \emph  {i.e.} the diverging term $V_P^\textsc  {c}$ comes from ``variance due to sampling''. (d,i) A bivariate symmetric decomposition of the variance resolves this ambiguity and shows that the diverging term is actually $V_{PD}$, \emph  {i.e.} ``the variance explained by the parameters and data together beyond what they explain individually.'' (e,j) A trivariate symmetric decomposition reveals that the divergence comes from two terms, $V_{PX}$ and $V_{PX\bm  {\varepsilon }}$ (outlined in dashed red), and shows that label noise exacerbates but does not cause double descent. Since $V_{\varepsilon }=V_{P{\varepsilon }}=0$, they are not shown in (j). Taken from \cite  {adlam2020understandingdoubledescentrequires}}}{17}{figure.caption.6}\protected@file@percent }
\newlabel{fig:venn_variance}{{6}{17}{(\textbf {a-e) The different bias-variance decompositions.} (f-j) Corresponding theoretical predictions for $\gamma =0$, $\phi =1/16$ and $\fs = \tanh $ with $\text {SNR} = 100$ as the model capacity varies across the interpolation threshold (dashed red). (a,f) The semi-classical decomposition of~\cite {hastie2019surprises,mei2019generalization} has a nonmonotonic and divergent bias term, conflicting with standard definitions of the bias. (b,g) The decomposition of~\cite {neal2018modern} utilizing the law of total variance interprets the diverging term $V_D^\textsc {c}$ as ``variance due to optimization''. (c,h) An alternative application of the law of total variance suggests the opposite, \emph {i.e.} the diverging term $V_P^\textsc {c}$ comes from ``variance due to sampling''. (d,i) A bivariate symmetric decomposition of the variance resolves this ambiguity and shows that the diverging term is actually $V_{PD}$, \emph {i.e.} ``the variance explained by the parameters and data together beyond what they explain individually.'' (e,j) A trivariate symmetric decomposition reveals that the divergence comes from two terms, $V_{PX}$ and $V_{PX\bfe }$ (outlined in dashed red), and shows that label noise exacerbates but does not cause double descent. Since $V_\e =V_{P\e }=0$, they are not shown in (j). Taken from \cite {adlam2020understandingdoubledescentrequires}}{figure.caption.6}{}}
\citation{belkin_reconciling_2019}
\@writefile{toc}{\contentsline {section}{\numberline {7}Double descent}{18}{section.7}\protected@file@percent }
\citation{belkin_reconciling_2019}
\citation{belkin_reconciling_2019}
\citation{belkin_reconciling_2019}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces {\bf  Curves for training risk (dashed line) and test risk (solid line).} ({\bf  a}) The classical \emph  {U-shaped risk curve} arising from the bias-variance trade-off. ({\bf  b}) The \emph  {double descent risk curve}, which incorporates the U-shaped risk curve (i.e., the ``classical'' regime) together with the observed behaviour from using high capacity function classes (i.e., the ``modern'' interpolating regime), separated by the interpolation threshold. The predictors to the right of the interpolation threshold have zero training risk. Reproduced from \cite  {belkin_reconciling_2019}.}}{19}{figure.caption.7}\protected@file@percent }
\newlabel{fig:double-descent}{{7}{19}{{\bf Curves for training risk (dashed line) and test risk (solid line).} ({\bf a}) The classical \emph {U-shaped risk curve} arising from the bias-variance trade-off. ({\bf b}) The \emph {double descent risk curve}, which incorporates the U-shaped risk curve (i.e., the ``classical'' regime) together with the observed behaviour from using high capacity function classes (i.e., the ``modern'' interpolating regime), separated by the interpolation threshold. The predictors to the right of the interpolation threshold have zero training risk. Reproduced from \cite {belkin_reconciling_2019}}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces {\bf  Left:} Train and test error as a function of model size, for ResNet18s of varying width on CIFAR-10 with 15\% label noise. {\bf  Right:} Test error, shown for varying train epochs. All models trained using Adam for 4K epochs. The largest model (width $64$) corresponds to standard ResNet18. Resued from \cite  {nakkiran_deep_2019}. }}{20}{figure.caption.8}\protected@file@percent }
\newlabel{fig:errorvscomplexity}{{8}{20}{{\bf Left:} Train and test error as a function of model size, for ResNet18s of varying width on CIFAR-10 with 15\% label noise. {\bf Right:} Test error, shown for varying train epochs. All models trained using Adam for 4K epochs. The largest model (width $64$) corresponds to standard ResNet18. Resued from \cite {nakkiran_deep_2019}}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces {\bf  Left:} Test error as a function of model size and train epochs. The horizontal line corresponds to model-wise double descent--varying model size while training for as long as possible. The vertical line corresponds to epoch-wise double descent, with test error undergoing double-descent as train time increases. {\bf  Right} Train error of the corresponding models. All models are Resnet18s trained on CIFAR-10 with 15\% label noise, data-augmentation, and Adam for up to 4K epochs. Reused from \cite  {nakkiran_deep_2019}}}{20}{figure.caption.9}\protected@file@percent }
\newlabel{fig:unified}{{9}{20}{{\bf Left:} Test error as a function of model size and train epochs. The horizontal line corresponds to model-wise double descent--varying model size while training for as long as possible. The vertical line corresponds to epoch-wise double descent, with test error undergoing double-descent as train time increases. {\bf Right} Train error of the corresponding models. All models are Resnet18s trained on CIFAR-10 with 15\% label noise, data-augmentation, and Adam for up to 4K epochs. Reused from \cite {nakkiran_deep_2019}}{figure.caption.9}{}}
\@writefile{loe}{\contentsline {hypothesis}{\ifthmt@listswap Hypothesis~7.1\else \numberline {7.1}Hypothesis\fi \thmtformatoptarg {Generalized Double Descent hypothesis, informal}}{20}{hypothesis.1}\protected@file@percent }
\newlabel{hyp:informaldd}{{7.1}{20}{Generalized Double Descent hypothesis, informal}{hypothesis.1}{}}
\citation{nakkiran_deep_2019}
\citation{belkin_reconciling_2019}
\citation{advani2017highdimensionaldynamicsgeneralizationerror}
\citation{belkin2018understanddeeplearningneed}
\citation{mei2020generalizationerrorrandomfeatures}
\citation{shi2024homophilymodulatesdoubledescent}
\citation{hu2021modelcomplexitydeeplearning,luo2024investigatingimpactmodelcomplexity,barceló2020modelinterpretabilitylenscomputational,Molnar_2020,janik2021complexitydeepneuralnetworks}
\citation{hu2021modelcomplexitydeeplearning}
\citation{Molnar_2020}
\citation{belkin_reconciling_2019}
\@writefile{toc}{\contentsline {section}{\numberline {8}Pre-analysis of double descent}{22}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Model complexity}{22}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Model structure}{22}{subsection.8.2}\protected@file@percent }
\citation{nakkiran_deep_2019}
\citation{10.5555/2371238}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Setting consistency}{23}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Classical analytical difficulty}{23}{subsection.8.4}\protected@file@percent }
\citation{domingos_unifeid_2000}
\citation{brown2024biasvariance}
\citation{lafon_understanding_2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Hypothetical analysis}{24}{subsection.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.1}Stability of bias-variance measure}{24}{subsubsection.8.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.2}Alternative measures}{24}{subsubsection.8.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.3}Inductive bias}{24}{subsubsection.8.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.4}Randomness and probabilistic setting}{24}{subsubsection.8.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.5}Knowledge masking}{25}{subsubsection.8.5.5}\protected@file@percent }
\citation{10.5555/200548}
\citation{Vapnik1999-VAPTNO}
\@writefile{toc}{\contentsline {section}{\numberline {9}Pre-experimental analysis}{26}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Complexity measures}{26}{subsection.9.1}\protected@file@percent }
\citation{STL_Hajek_Maxim_2021,10.5555/2371238,10.5555/2621980}
\citation{10.5555/200548}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Description of various complexity measure} that can be taken over a given model, and the specific learning process.}}{28}{table.caption.10}\protected@file@percent }
\citation{Wegener1987,MiltersenRadhakrishnanWegener2005,DarwicheMarquis2002}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.1}Representation complexity}{29}{subsubsection.9.1.1}\protected@file@percent }
\@writefile{loe}{\contentsline {assumption}{\ifthmt@listswap Assumption~15\else \numberline {15}Assumption\fi }{29}{assumption.15}\protected@file@percent }
\citation{6797087}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.2}Optimization complexity}{30}{subsubsection.9.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Learning process}{30}{subsection.9.2}\protected@file@percent }
\citation{10.5555/2371238}
\citation{10.1145/1968.1972,VapnikChervonenkis:1971}
\citation{BartlettMendelson:2002:Rademacher,BartlettBousquetMendelson:2005:LocalRademacher}
\citation{BousquetElisseeff:2002:Stability}
\citation{McAllester:1999:PACBayes}
\citation{FloydWarmuth:1995:SampleCompression}
\citation{RussoZou:2016:InformationTheory,XuRaginsky:2017:InfoGen}
\citation{Jacot:2018:NTK}
\citation{10.5555/200548}
\citation{Bartlett:1998:MarginComplexity,XuMannor:2010:RobustnessGeneralization,LittlestoneWarmuth:1994:WeightedMajority}
\citation{10.5555/2621980,10.5555/2371238,STL_Hajek_Maxim_2021}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1}PAC-learning procedure}{31}{subsubsection.9.2.1}\protected@file@percent }
\citation{10.5555/2371238}
\citation{10.5555/200548}
\newlabel{thm:theorem_inconsistent}{{20}{33}{Learning bound - finite $\mathcal {H}$, inconsistent case}{theorem.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.2}Occam learning}{33}{subsubsection.9.2.2}\protected@file@percent }
\citation{10.5555/2371238,STL_Hajek_Maxim_2021,10.5555/2930837}
\newlabel{eq:Occam1}{{22}{34}{Occam's Razor}{theorem.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Rademacher + VC pair}{34}{subsection.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Remark}{35}{subsection.9.4}\protected@file@percent }
\citation{Vapnik1999-VAPTNO}
\citation{Vapnik1999-VAPTNO}
\citation{Vapnik1999-VAPTNO}
\@writefile{toc}{\contentsline {section}{\numberline {10}Analytical experiments}{36}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Analytical model}{36}{subsection.10.1}\protected@file@percent }
\newlabel{SC@3}{{\caption@xref {??}{ on input line 831}}{36}{}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A model of learning from examples. During the learning process, the learning machine observes the pairs $(x, y)$ (the training set). After training, the machine must on any given return a value $j$. The goal is to return a value $y$ that is close to the supervisor's response $y$. We will use this interpretation in the experimental setting. Reused from \cite  {Vapnik1999-VAPTNO}}}{36}{figure.caption.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Observation 1 - Univariate standard function models}{36}{subsection.10.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \textbf  {Diagramatic view for the ambiance space of the modelling scenario setting.} Under classical learning theory and consideration, such scheme is used for almost every aspect possible of the learner's action. $c\in \mathcal  {C}, h\in \mathcal  {H}$ described by the tuple $(\mathbf  {w},\mathbf  {s})$ of main (weight) parameters and special parameters (for example, bias), $\mathbf  {I}$ as the input space. The 3-tuple $(\omega , k,m)$ is used for controlling the partitioning (for $k=2$ is the train-test split). Others include $P(\mathbf  {I},c,h,\mathcal  {D}_{c})$ as the theorized action sequence (where the concept is exhibited, or the where the observational space is formed) of supposed distribution $\mathcal  {D}_{c}$, the supervisor $\text  {Supervisor}(\nabla , \mathcal  {A}, \{\Theta _{S}\})$ for the loss class $\nabla $, algorithm $\mathcal  {A}$, and $\{\Theta _{S}\}$ of special parameters for the supervisor. Additionally, we also include the supposed randomized state generation, $\mathsf  {RAND}_{i}(\theta _{i,j,k})$ of distinct controlling parameters and arbitrary pseudo-random shape.}}{37}{figure.caption.12}\protected@file@percent }
\newlabel{fig:vapnik_scheme}{{11}{37}{\textbf {Diagramatic view for the ambiance space of the modelling scenario setting.} Under classical learning theory and consideration, such scheme is used for almost every aspect possible of the learner's action. $c\in \mathcal {C}, h\in \mathcal {H}$ described by the tuple $(\mathbf {w},\mathbf {s})$ of main (weight) parameters and special parameters (for example, bias), $\mathbf {I}$ as the input space. The 3-tuple $(\omega , k,m)$ is used for controlling the partitioning (for $k=2$ is the train-test split). Others include $P(\mathbf {I},c,h,\mathcal {D}_{c})$ as the theorized action sequence (where the concept is exhibited, or the where the observational space is formed) of supposed distribution $\mathcal {D}_{c}$, the supervisor $\text {Supervisor}(\nabla , \mathcal {A}, \{\Theta _{S}\})$ for the loss class $\nabla $, algorithm $\mathcal {A}$, and $\{\Theta _{S}\}$ of special parameters for the supervisor. Additionally, we also include the supposed randomized state generation, $\mathsf {RAND}_{i}(\theta _{i,j,k})$ of distinct controlling parameters and arbitrary pseudo-random shape}{figure.caption.12}{}}
\citation{lafon_understanding_2024}
\citation{lafon_understanding_2024}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Elementary function classes and their respective parameter-control / compound forms on unitary $\mathbb  {R}$-space}. For linear we do not expand into a compound; polynomials admit coefficient convolution under multiplication, exponentials admit sums of exponentials (Proxy / exponential sums), and trigonometric functions admit Fourier-series decompositions. Analytical expression and complexity analysis of exponential and trigonometric class can be reduced to singular network, however their expressive complexity is harder to define. }}{38}{table.caption.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.1}Linear vs all}{38}{subsubsection.10.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.2}Polynomial-vs-all}{39}{subsubsection.10.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.3}Exponential compound model}{39}{subsubsection.10.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.4}Fourier-based compound model}{39}{subsubsection.10.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Observation 2 - Prescription from \cite  {lafon_understanding_2024}}{39}{subsection.10.3}\protected@file@percent }
\newlabel{eq:linear_gaussian_erm}{{38}{39}{}{equation.38}{}}
\newlabel{thm:double_descent_lr}{{30}{39}{Double descent on linear regression}{theorem.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Similar experiment according to setting and result of Theorem~\ref {eq:linear_gaussian_erm}, where $p$ is in the range [1,5523]. Here, we can see small double descent modelled exactly at the interpolation threshold, while the later region exhibit similar phenomenon as bias-variance tradeoff.}}{41}{figure.caption.14}\protected@file@percent }
\newlabel{fig:contrarian}{{12}{41}{Similar experiment according to setting and result of Theorem~\ref {eq:linear_gaussian_erm}, where $p$ is in the range [1,5523]. Here, we can see small double descent modelled exactly at the interpolation threshold, while the later region exhibit similar phenomenon as bias-variance tradeoff}{figure.caption.14}{}}
\newlabel{fig:1a}{{13a}{42}{$n=12$}{figure.caption.15}{}}
\newlabel{sub@fig:1a}{{a}{42}{$n=12$}{figure.caption.15}{}}
\newlabel{fig:1b}{{13b}{42}{$n=24$}{figure.caption.15}{}}
\newlabel{sub@fig:1b}{{b}{42}{$n=24$}{figure.caption.15}{}}
\newlabel{fig:1c}{{13c}{42}{$n=36$}{figure.caption.15}{}}
\newlabel{sub@fig:1c}{{c}{42}{$n=36$}{figure.caption.15}{}}
\newlabel{fig:1d}{{13d}{42}{$n=50$}{figure.caption.15}{}}
\newlabel{sub@fig:1d}{{d}{42}{$n=50$}{figure.caption.15}{}}
\newlabel{fig:2a}{{13e}{42}{$n=76$}{figure.caption.15}{}}
\newlabel{sub@fig:2a}{{e}{42}{$n=76$}{figure.caption.15}{}}
\newlabel{fig:2b}{{13f}{42}{$n=82$}{figure.caption.15}{}}
\newlabel{sub@fig:2b}{{f}{42}{$n=82$}{figure.caption.15}{}}
\newlabel{fig:2c}{{13g}{42}{$n=90$}{figure.caption.15}{}}
\newlabel{sub@fig:2c}{{g}{42}{$n=90$}{figure.caption.15}{}}
\newlabel{fig:2d}{{13h}{42}{$n=100$}{figure.caption.15}{}}
\newlabel{sub@fig:2d}{{h}{42}{$n=100$}{figure.caption.15}{}}
\newlabel{fig:3a}{{13i}{42}{$n=106$}{figure.caption.15}{}}
\newlabel{sub@fig:3a}{{i}{42}{$n=106$}{figure.caption.15}{}}
\newlabel{fig:3b}{{13j}{42}{$n=112$}{figure.caption.15}{}}
\newlabel{sub@fig:3b}{{j}{42}{$n=112$}{figure.caption.15}{}}
\newlabel{fig:3c}{{13k}{42}{$n=121$}{figure.caption.15}{}}
\newlabel{sub@fig:3c}{{k}{42}{$n=121$}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Theorem~\ref {thm:double_descent_lr} behaviours on randomized setting. For this model, we have $d=100$, $\sigma = 0.5$, and variational $n$. Full test cases are for $p=[1,100]$, $n=\{12,24,36,50,76,82,90,100,106,112,121\}$ accordingly. The test function of the concept itself is the same linear model, but with different input only.}}{42}{figure.caption.15}\protected@file@percent }
\newlabel{fig:11grid}{{13}{42}{Theorem~\ref {thm:double_descent_lr} behaviours on randomized setting. For this model, we have $d=100$, $\sigma = 0.5$, and variational $n$. Full test cases are for $p=[1,100]$, $n=\{12,24,36,50,76,82,90,100,106,112,121\}$ accordingly. The test function of the concept itself is the same linear model, but with different input only}{figure.caption.15}{}}
\newlabel{fig:1a2}{{14a}{43}{$n=12$}{figure.caption.16}{}}
\newlabel{sub@fig:1a2}{{a}{43}{$n=12$}{figure.caption.16}{}}
\newlabel{fig:1b2}{{14b}{43}{$n=24$}{figure.caption.16}{}}
\newlabel{sub@fig:1b2}{{b}{43}{$n=24$}{figure.caption.16}{}}
\newlabel{fig:1c2}{{14c}{43}{$n=36$}{figure.caption.16}{}}
\newlabel{sub@fig:1c2}{{c}{43}{$n=36$}{figure.caption.16}{}}
\newlabel{fig:1d2}{{14d}{43}{$n=50$}{figure.caption.16}{}}
\newlabel{sub@fig:1d2}{{d}{43}{$n=50$}{figure.caption.16}{}}
\newlabel{fig:2a2}{{14e}{43}{$n=76$}{figure.caption.16}{}}
\newlabel{sub@fig:2a2}{{e}{43}{$n=76$}{figure.caption.16}{}}
\newlabel{fig:2b2}{{14f}{43}{$n=82$}{figure.caption.16}{}}
\newlabel{sub@fig:2b2}{{f}{43}{$n=82$}{figure.caption.16}{}}
\newlabel{fig:2c2}{{14g}{43}{$n=90$}{figure.caption.16}{}}
\newlabel{sub@fig:2c2}{{g}{43}{$n=90$}{figure.caption.16}{}}
\newlabel{fig:2d2}{{14h}{43}{$n=100$}{figure.caption.16}{}}
\newlabel{sub@fig:2d2}{{h}{43}{$n=100$}{figure.caption.16}{}}
\newlabel{fig:3a2}{{14i}{43}{$n=106$}{figure.caption.16}{}}
\newlabel{sub@fig:3a2}{{i}{43}{$n=106$}{figure.caption.16}{}}
\newlabel{fig:3b2}{{14j}{43}{$n=112$}{figure.caption.16}{}}
\newlabel{sub@fig:3b2}{{j}{43}{$n=112$}{figure.caption.16}{}}
\newlabel{fig:3c2}{{14k}{43}{$n=121$}{figure.caption.16}{}}
\newlabel{sub@fig:3c2}{{k}{43}{$n=121$}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Contrary to Theorem~\ref {thm:double_descent_lr}, this is the behaviours on randomized setting for standard parameter-wised mean square error measure (MSE-on-parameter). For this model, we have $d=100$, $\sigma = 0.5$, and variational $n$. Full test cases are for $p=[1,100]$, $n=\{12,24,36,50,76,82,90,100,106,112,121\}$ accordingly. The test function of the concept itself is the same linear model, but with different input only.}}{43}{figure.caption.16}\protected@file@percent }
\newlabel{fig:11grid2}{{14}{43}{Contrary to Theorem~\ref {thm:double_descent_lr}, this is the behaviours on randomized setting for standard parameter-wised mean square error measure (MSE-on-parameter). For this model, we have $d=100$, $\sigma = 0.5$, and variational $n$. Full test cases are for $p=[1,100]$, $n=\{12,24,36,50,76,82,90,100,106,112,121\}$ accordingly. The test function of the concept itself is the same linear model, but with different input only}{figure.caption.16}{}}
\newlabel{fig:1a21}{{15a}{44}{$n=12$}{figure.caption.17}{}}
\newlabel{sub@fig:1a21}{{a}{44}{$n=12$}{figure.caption.17}{}}
\newlabel{fig:1b21}{{15b}{44}{$n=24$}{figure.caption.17}{}}
\newlabel{sub@fig:1b21}{{b}{44}{$n=24$}{figure.caption.17}{}}
\newlabel{fig:1c21}{{15c}{44}{$n=36$}{figure.caption.17}{}}
\newlabel{sub@fig:1c21}{{c}{44}{$n=36$}{figure.caption.17}{}}
\newlabel{fig:1d21}{{15d}{44}{$n=50$}{figure.caption.17}{}}
\newlabel{sub@fig:1d21}{{d}{44}{$n=50$}{figure.caption.17}{}}
\newlabel{fig:2a21}{{15e}{44}{$n=76$}{figure.caption.17}{}}
\newlabel{sub@fig:2a21}{{e}{44}{$n=76$}{figure.caption.17}{}}
\newlabel{fig:2b21}{{15f}{44}{$n=82$}{figure.caption.17}{}}
\newlabel{sub@fig:2b21}{{f}{44}{$n=82$}{figure.caption.17}{}}
\newlabel{fig:2c21}{{15g}{44}{$n=90$}{figure.caption.17}{}}
\newlabel{sub@fig:2c21}{{g}{44}{$n=90$}{figure.caption.17}{}}
\newlabel{fig:2d21}{{15h}{44}{$n=100$}{figure.caption.17}{}}
\newlabel{sub@fig:2d21}{{h}{44}{$n=100$}{figure.caption.17}{}}
\newlabel{fig:3a21}{{15i}{44}{$n=106$}{figure.caption.17}{}}
\newlabel{sub@fig:3a21}{{i}{44}{$n=106$}{figure.caption.17}{}}
\newlabel{fig:3b21}{{15j}{44}{$n=112$}{figure.caption.17}{}}
\newlabel{sub@fig:3b21}{{j}{44}{$n=112$}{figure.caption.17}{}}
\newlabel{fig:3c21}{{15k}{44}{$n=121$}{figure.caption.17}{}}
\newlabel{sub@fig:3c21}{{k}{44}{$n=121$}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Contrary to Theorem~\ref {thm:double_descent_lr} and both figure~\ref {fig:11grid2}, this is the behaviours on randomized setting for standard parameter-wised mean square error measure (MSE-on-prediction). Especially, we do not see double descent pattern in this setting, as it is. For this model, we have $d=100$, $\sigma = 0.5$, and variational $n$. Full test cases are for $p=[1,100]$, $n=\{12,24,36,50,76,82,90,100,106,112,121\}$ accordingly. The test function of the concept itself is the same linear model, but with different input only.}}{44}{figure.caption.17}\protected@file@percent }
\newlabel{fig:11grid21}{{15}{44}{Contrary to Theorem~\ref {thm:double_descent_lr} and both figure~\ref {fig:11grid2}, this is the behaviours on randomized setting for standard parameter-wised mean square error measure (MSE-on-prediction). Especially, we do not see double descent pattern in this setting, as it is. For this model, we have $d=100$, $\sigma = 0.5$, and variational $n$. Full test cases are for $p=[1,100]$, $n=\{12,24,36,50,76,82,90,100,106,112,121\}$ accordingly. The test function of the concept itself is the same linear model, but with different input only}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.1}Verification}{45}{subsubsection.10.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.2}Remark}{45}{subsubsection.10.3.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Linear regression with Gaussian white noise, full control.}}{46}{algocf.1}\protected@file@percent }
\newlabel{algo:algo_reg_lin_1}{{1}{46}{}{algocf.1}{}}
\newlabel{fig:2a}{{16a}{47}{$n=12$}{figure.caption.19}{}}
\newlabel{sub@fig:2a}{{a}{47}{$n=12$}{figure.caption.19}{}}
\newlabel{fig:2b}{{16b}{47}{$n=24$}{figure.caption.19}{}}
\newlabel{sub@fig:2b}{{b}{47}{$n=24$}{figure.caption.19}{}}
\newlabel{fig:2c}{{16c}{47}{$n=36$}{figure.caption.19}{}}
\newlabel{sub@fig:2c}{{c}{47}{$n=36$}{figure.caption.19}{}}
\newlabel{fig:2d}{{16d}{47}{$n=50$}{figure.caption.19}{}}
\newlabel{sub@fig:2d}{{d}{47}{$n=50$}{figure.caption.19}{}}
\newlabel{fig:2e}{{16e}{47}{$n=76$}{figure.caption.19}{}}
\newlabel{sub@fig:2e}{{e}{47}{$n=76$}{figure.caption.19}{}}
\newlabel{fig:2f}{{16f}{47}{$n=82$}{figure.caption.19}{}}
\newlabel{sub@fig:2f}{{f}{47}{$n=82$}{figure.caption.19}{}}
\newlabel{fig:2g}{{16g}{47}{$n=90$}{figure.caption.19}{}}
\newlabel{sub@fig:2g}{{g}{47}{$n=90$}{figure.caption.19}{}}
\newlabel{fig:2h}{{16h}{47}{$n=100$}{figure.caption.19}{}}
\newlabel{sub@fig:2h}{{h}{47}{$n=100$}{figure.caption.19}{}}
\newlabel{fig:2i}{{16i}{47}{$n=106$}{figure.caption.19}{}}
\newlabel{sub@fig:2i}{{i}{47}{$n=106$}{figure.caption.19}{}}
\newlabel{fig:2j}{{16j}{47}{$n=112$}{figure.caption.19}{}}
\newlabel{sub@fig:2j}{{j}{47}{$n=112$}{figure.caption.19}{}}
\newlabel{fig:2k}{{16k}{47}{$n=121$}{figure.caption.19}{}}
\newlabel{sub@fig:2k}{{k}{47}{$n=121$}{figure.caption.19}{}}
\newlabel{fig:2l}{{16l}{47}{$n=150$}{figure.caption.19}{}}
\newlabel{sub@fig:2l}{{l}{47}{$n=150$}{figure.caption.19}{}}
\newlabel{fig:2m}{{16m}{47}{$n=231$}{figure.caption.19}{}}
\newlabel{sub@fig:2m}{{m}{47}{$n=231$}{figure.caption.19}{}}
\newlabel{fig:2n}{{16n}{47}{$n=250$}{figure.caption.19}{}}
\newlabel{sub@fig:2n}{{n}{47}{$n=250$}{figure.caption.19}{}}
\newlabel{fig:2o}{{16o}{47}{$n=256$}{figure.caption.19}{}}
\newlabel{sub@fig:2o}{{o}{47}{$n=256$}{figure.caption.19}{}}
\newlabel{fig:2p}{{16p}{47}{$n=280$}{figure.caption.19}{}}
\newlabel{sub@fig:2p}{{p}{47}{$n=280$}{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Risk curves for varying sample size $n$ (dimension $d=100$, noise $\sigma =0.5$). Double descent is determined in similar sequence, even though the interpolation theoretical $p=n$ is actually shifted over to the right from the actual interpolation observables, in parameterization. Afterward, correlation fails, and the theoretical $p$ is ineffective, similar to previous theorem-based test run. Sample set size range is $n\in \{12,24,36,50,76,82,90,100,106,112,121,150,231,250,256,280\}$. One additional remark is that the error landscape is relatively thin in either side, making it abnormal in comparison to actual bias-variance curvature.}}{47}{figure.caption.19}\protected@file@percent }
\newlabel{fig:risk_all_grid}{{16}{47}{Risk curves for varying sample size $n$ (dimension $d=100$, noise $\sigma =0.5$). Double descent is determined in similar sequence, even though the interpolation theoretical $p=n$ is actually shifted over to the right from the actual interpolation observables, in parameterization. Afterward, correlation fails, and the theoretical $p$ is ineffective, similar to previous theorem-based test run. Sample set size range is $n\in \{12,24,36,50,76,82,90,100,106,112,121,150,231,250,256,280\}$. One additional remark is that the error landscape is relatively thin in either side, making it abnormal in comparison to actual bias-variance curvature}{figure.caption.19}{}}
\citation{goodfellow2016deep}
\citation{gareth_james_introduction_2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Observation 3 - Polynomial regression}{48}{subsection.10.4}\protected@file@percent }
\@writefile{loe}{\contentsline {setting}{\ifthmt@listswap Setting~10.1\else \numberline {10.1}Setting\fi \thmtformatoptarg {Polynomial testing}}{49}{setting.1}\protected@file@percent }
\citation{Vapnik1999-VAPTNO}
\citation{Cristianini2000AnIT}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Observation 5 - Support Vector Machine}{50}{subsection.10.5}\protected@file@percent }
\@writefile{loe}{\contentsline {setting}{\ifthmt@listswap Setting~10.2\else \numberline {10.2}Setting\fi \thmtformatoptarg {Support vector machine}}{51}{setting.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces \textbf  {Experiment on support vector machine on parameterized double descent identification}. The pattern appears marginally on the result graph, with a correct case of interpolation prediction for $p=n$.}}{51}{figure.caption.20}\protected@file@percent }
\newlabel{fig:test_svm_com_ideal}{{17}{51}{\textbf {Experiment on support vector machine on parameterized double descent identification}. The pattern appears marginally on the result graph, with a correct case of interpolation prediction for $p=n$}{figure.caption.20}{}}
\newlabel{fig:2a1}{{18a}{52}{$n=12$}{figure.caption.21}{}}
\newlabel{sub@fig:2a1}{{a}{52}{$n=12$}{figure.caption.21}{}}
\newlabel{fig:2b1}{{18b}{52}{$n=24$}{figure.caption.21}{}}
\newlabel{sub@fig:2b1}{{b}{52}{$n=24$}{figure.caption.21}{}}
\newlabel{fig:2c1}{{18c}{52}{$n=36$}{figure.caption.21}{}}
\newlabel{sub@fig:2c1}{{c}{52}{$n=36$}{figure.caption.21}{}}
\newlabel{fig:2d1}{{18d}{52}{$n=50$}{figure.caption.21}{}}
\newlabel{sub@fig:2d1}{{d}{52}{$n=50$}{figure.caption.21}{}}
\newlabel{fig:2e1}{{18e}{52}{$n=76$}{figure.caption.21}{}}
\newlabel{sub@fig:2e1}{{e}{52}{$n=76$}{figure.caption.21}{}}
\newlabel{fig:2f1}{{18f}{52}{$n=82$}{figure.caption.21}{}}
\newlabel{sub@fig:2f1}{{f}{52}{$n=82$}{figure.caption.21}{}}
\newlabel{fig:2g1}{{18g}{52}{$n=90$}{figure.caption.21}{}}
\newlabel{sub@fig:2g1}{{g}{52}{$n=90$}{figure.caption.21}{}}
\newlabel{fig:2h1}{{18h}{52}{$n=100$}{figure.caption.21}{}}
\newlabel{sub@fig:2h1}{{h}{52}{$n=100$}{figure.caption.21}{}}
\newlabel{fig:2i}{{18i}{52}{$n=106$}{figure.caption.21}{}}
\newlabel{sub@fig:2i}{{i}{52}{$n=106$}{figure.caption.21}{}}
\newlabel{fig:2j1}{{18j}{52}{$n=112$}{figure.caption.21}{}}
\newlabel{sub@fig:2j1}{{j}{52}{$n=112$}{figure.caption.21}{}}
\newlabel{fig:2k1}{{18k}{52}{$n=121$}{figure.caption.21}{}}
\newlabel{sub@fig:2k1}{{k}{52}{$n=121$}{figure.caption.21}{}}
\newlabel{fig:2l1}{{18l}{52}{$n=150$}{figure.caption.21}{}}
\newlabel{sub@fig:2l1}{{l}{52}{$n=150$}{figure.caption.21}{}}
\newlabel{fig:2m1}{{18m}{52}{$n=231$}{figure.caption.21}{}}
\newlabel{sub@fig:2m1}{{m}{52}{$n=231$}{figure.caption.21}{}}
\newlabel{fig:2n1}{{18n}{52}{$n=250$}{figure.caption.21}{}}
\newlabel{sub@fig:2n1}{{n}{52}{$n=250$}{figure.caption.21}{}}
\newlabel{fig:2o1}{{18o}{52}{$n=256$}{figure.caption.21}{}}
\newlabel{sub@fig:2o1}{{o}{52}{$n=256$}{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces \textbf  {Risk plots of support vector machine on incremental $p$ and $n$}. The pattern is not interpretable, albeit we have doubts about the actual result.}}{52}{figure.caption.21}\protected@file@percent }
\newlabel{fig:risk_all_grid_svm}{{18}{52}{\textbf {Risk plots of support vector machine on incremental $p$ and $n$}. The pattern is not interpretable, albeit we have doubts about the actual result}{figure.caption.21}{}}
\citation{zhang2023mathematical}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6}Neural network}{53}{subsection.10.6}\protected@file@percent }
\@writefile{loe}{\contentsline {setting}{\ifthmt@listswap Setting~10.3\else \numberline {10.3}Setting\fi }{53}{setting.3}\protected@file@percent }
\citation{mcculloch_logical_1943,Rosenblatt1958ThePA}
\@writefile{toc}{\contentsline {section}{\numberline {11}Theoretical analysis}{54}{section.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Motivation}{54}{subsection.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Physics-inspired perspective 1}{55}{section.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}A Formal Link via PAC-Bayesian Bounds}{55}{subsection.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Topological Criticality in Graph Neural Networks}{56}{section.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}The Interpolation Peak as a Critical Phenomenon}{56}{subsection.13.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of Criticality Mechanisms}}{57}{table.caption.22}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}Physics-inspired perspective 2}{57}{section.14}\protected@file@percent }
\citation{shi2024homophilymodulatesdoubledescent}
\citation{shi2024homophilymodulatesdoubledescent,buschjager_generalized_2020}
\citation{GRP_Hamilton}
\citation{shi2024homophilymodulatesdoubledescent}
\citation{shi2024homophilymodulatesdoubledescent}
\@writefile{toc}{\contentsline {section}{\numberline {15}Experiments 5 - Graph Neural Networks}{58}{section.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1}Methodology}{58}{subsection.15.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.1.1}Quick introduction to graph theory}{58}{subsubsection.15.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  \textbf  {Illustration of an ordering for a specific graph configuration.} We emphasize the perspective to the higher neighbouring degree node in the graph, namely $V_{1}$, and their relative links and connections of the graph itself. A 2-walk (in red) can be seen, which aggregates more to create the 2-neighbourhood of maximally two walks away. }}{59}{figure.caption.23}\protected@file@percent }
\citation{GRP_Hamilton}
\citation{Oono2020Graph,lopushanskyy2024graphneuralnetworksgraph,Scar04,GRP_Hamilton}
\citation{rossi2020temporalgraphnetworksdeep}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Illustrative decomposition of the graph on its simplexes edges and vertices: Each of the decomposed space $\mathcal  {V}$ and $\mathcal  {E}$ can be encoded separately to their own ordeal, for example, of the edge connection through incident matrix.}}{60}{figure.caption.24}\protected@file@percent }
\newlabel{fig:graphdecom}{{20}{60}{Illustrative decomposition of the graph on its simplexes edges and vertices: Each of the decomposed space $\mathcal {V}$ and $\mathcal {E}$ can be encoded separately to their own ordeal, for example, of the edge connection through incident matrix}{figure.caption.24}{}}
\citation{GRP_Hamilton,Scar04}
\citation{Scar04,Veli_kovi__2023,tanis2024introductiongraphneuralnetworks,lopushanskyy2024graphneuralnetworksgraph}
\citation{pyg_docs,Fey/Lenssen/2019}
\citation{Scar04,GRP_Hamilton}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.1.2}Graph Neural Network}{61}{subsubsection.15.1.2}\protected@file@percent }
\citation{GRP_Hamilton}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces A conceptual illustration on the running flow of an $n$-layer GNN on particular structure of interest. Note that the data section itself has particular embedding structure on its own.}}{62}{figure.caption.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Encoding space and the change of encoding inbetween a static graph (or snapshot-wise) GNN. The encoding space is warped toward arbitrary notions inside a GNN, to the point that after certain layers of processing, the encoding outputs different from expected encoding space (native to the model, not to the designer), though there might be universal notions preserved, like the degree of node represented in a different way.}}{62}{figure.caption.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2}Double descent and GNN}{63}{subsection.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3}Testing scenario}{63}{subsection.15.3}\protected@file@percent }
\@writefile{loe}{\contentsline {question}{\ifthmt@listswap Question~15.1\else \numberline {15.1}Question\fi }{63}{question.1}\protected@file@percent }
\bibdata{references}
\bibcite{unified_bias_composition}{{1}{}{{uni}}{{}}}
\bibcite{achlioptas_stochastic_nodate}{{2}{}{{Achlioptas}}{{}}}
\bibcite{adlam2020understandingdoubledescentrequires}{{3}{2020}{{Adlam and Pennington}}{{}}}
\bibcite{advani2017highdimensionaldynamicsgeneralizationerror}{{4}{2017}{{Advani and Saxe}}{{}}}
\bibcite{barceló2020modelinterpretabilitylenscomputational}{{5}{2020}{{Barceló et~al.}}{{Barceló, Monet, Pérez, and Subercaseaux}}}
\bibcite{Bartlett:1998:MarginComplexity}{{6}{1998}{{Bartlett}}{{}}}
\bibcite{BartlettMendelson:2002:Rademacher}{{7}{2002}{{Bartlett and Mendelson}}{{}}}
\bibcite{BartlettBousquetMendelson:2005:LocalRademacher}{{8}{2005}{{Bartlett et~al.}}{{Bartlett, Bousquet, and Mendelson}}}
\bibcite{belkin2018understanddeeplearningneed}{{9}{2018}{{Belkin et~al.}}{{Belkin, Ma, and Mandal}}}
\bibcite{belkin_reconciling_2019}{{10}{2019}{{Belkin et~al.}}{{Belkin, Hsu, Ma, and Mandal}}}
\bibcite{BousquetElisseeff:2002:Stability}{{11}{2002}{{Bousquet and Elisseeff}}{{}}}
\bibcite{bousquet2020theoryuniversallearning}{{12}{2020}{{Bousquet et~al.}}{{Bousquet, Hanneke, Moran, van Handel, and Yehudayoff}}}
\bibcite{Bronstein_2017}{{13}{2017}{{Bronstein et~al.}}{{Bronstein, Bruna, LeCun, Szlam, and Vandergheynst}}}
\bibcite{bronstein2021geometricdeeplearninggrids}{{14}{2021}{{Bronstein et~al.}}{{Bronstein, Bruna, Cohen, and Veličković}}}
\bibcite{brown2024biasvariance}{{15}{2024}{{Brown and Ali}}{{}}}
\bibcite{buschjager_generalized_2020}{{16}{2020}{{Buschjäger et~al.}}{{Buschjäger, Pfahler, and Morik}}}
\bibcite{Cristianini2000AnIT}{{17}{2000}{{Cristianini and Shawe-Taylor}}{{}}}
\bibcite{d_ascoli_triple_2020}{{18}{2020}{{d'~Ascoli et~al.}}{{d'~Ascoli, Sagun, and Biroli}}}
\bibcite{DarwicheMarquis2002}{{19}{2002}{{Darwiche and Marquis}}{{}}}
\bibcite{davies_unifying_2023}{{20}{2023}{{Davies et~al.}}{{Davies, Langosco, and Krueger}}}
\bibcite{Domingos2000AUB}{{21}{2000{a}}{{Domingos}}{{}}}
\bibcite{domingos_unifeid_2000}{{22}{2000{b}}{{Domingos}}{{}}}
\bibcite{LehmannCasella_theory_1998}{{23}{1998}{{E.~L.~Lehmann}}{{}}}
\bibcite{Fey/Lenssen/2019}{{24}{2019}{{Fey and Lenssen}}{{}}}
\bibcite{FloydWarmuth:1995:SampleCompression}{{25}{1995}{{Floyd and Warmuth}}{{}}}
\bibcite{Scott_Fortmann_Bias}{{26}{2012}{{Fortmann}}{{}}}
\bibcite{6797087}{{27}{1992}{{Geman et~al.}}{{Geman, Bienenstock, and Doursat}}}
\bibcite{goodfellow2016deep}{{28}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{STL_Hajek_Maxim_2021}{{29}{2021}{{Hajek and Raginsky}}{{}}}
\bibcite{GRP_Hamilton}{{30}{}{{Hamilton}}{{}}}
\bibcite{hastie2019surprises}{{31}{2019}{{Hastie et~al.}}{{Hastie, Montanari, Rosset, and Tibshirani}}}
\bibcite{hu2021modelcomplexitydeeplearning}{{32}{2021}{{Hu et~al.}}{{Hu, Chu, Pei, Liu, and Bian}}}
\bibcite{Jacot:2018:NTK}{{33}{2018}{{Jacot et~al.}}{{Jacot, Gabriel, and Hongler}}}
\bibcite{gareth_james_introduction_2013}{{34}{2013}{{James et~al.}}{{James, Hastie, Tibshirani, and Witten}}}
\bibcite{janik2021complexitydeepneuralnetworks}{{35}{2021}{{Janik and Witaszczyk}}{{}}}
\bibcite{MkayPretenceSignalStatistics1993}{{36}{1993}{{Kay}}{{}}}
\bibcite{10.5555/200548}{{37}{1994}{{Kearns and Vazirani}}{{}}}
\bibcite{lafon_understanding_2024}{{38}{2024}{{Lafon and Thomas}}{{}}}
\bibcite{LittlestoneWarmuth:1994:WeightedMajority}{{39}{1994}{{Littlestone and Warmuth}}{{}}}
\bibcite{liu2023understandingroleoptimizationdouble}{{40}{2023}{{Liu and Flanigan}}{{}}}
\bibcite{lopushanskyy2024graphneuralnetworksgraph}{{41}{2024}{{Lopushanskyy and Shi}}{{}}}
\bibcite{luo2024investigatingimpactmodelcomplexity}{{42}{2024}{{Luo et~al.}}{{Luo, Wang, and Huang}}}
\bibcite{McAllester:1999:PACBayes}{{43}{1999}{{McAllester}}{{}}}
\bibcite{mcculloch_logical_1943}{{44}{1943}{{McCulloch and Pitts}}{{}}}
\bibcite{mei2019generalization}{{45}{2019}{{Mei and Montanari}}{{}}}
\bibcite{mei2020generalizationerrorrandomfeatures}{{46}{2020}{{Mei and Montanari}}{{}}}
\bibcite{MiltersenRadhakrishnanWegener2005}{{47}{2005}{{Miltersen et~al.}}{{Miltersen, Radhakrishnan, and Wegener}}}
\bibcite{10.5555/2371238}{{48}{2012}{{Mohri et~al.}}{{Mohri, Rostamizadeh, and Talwalkar}}}
\bibcite{Molnar_2020}{{49}{2020}{{Molnar et~al.}}{{Molnar, Casalicchio, and Bischl}}}
\bibcite{nakkiran_deep_2019}{{50}{2019}{{Nakkiran et~al.}}{{Nakkiran, Kaplun, Bansal, Yang, Barak, and Sutskever}}}
\bibcite{neal2019biasvariancetradeofftextbooksneed}{{51}{2019}{{Neal}}{{}}}
\bibcite{neal2018modern}{{52}{2018}{{Neal et~al.}}{{Neal, Mittal, Baratin, Tantia, Scicluna, Lacoste-Julien, and Mitliagkas}}}
\bibcite{olmin2024understandingepochwisedoubledescent}{{53}{2024}{{Olmin and Lindsten}}{{}}}
\bibcite{Oono2020Graph}{{54}{2020}{{Oono and Suzuki}}{{}}}
\bibcite{liam_statistics_2005}{{55}{2005}{{Paninski}}{{}}}
\bibcite{PfauBregmanDivergence}{{56}{2013}{{Pfau}}{{}}}
\bibcite{piera_sample_2005}{{57}{2005}{{Piera and Javier}}{{}}}
\bibcite{pyg_docs}{{58}{2025}{{PyTorch Geometric Team}}{{}}}
\bibcite{Rosenblatt1958ThePA}{{59}{1958}{{Rosenblatt}}{{}}}
\bibcite{rossi2020temporalgraphnetworksdeep}{{60}{2020}{{Rossi et~al.}}{{Rossi, Chamberlain, Frasca, Eynard, Monti, and Bronstein}}}
\bibcite{ruder_overview_2017}{{61}{2017}{{Ruder}}{{}}}
\bibcite{RussoZou:2016:InformationTheory}{{62}{2016}{{Russo and Zou}}{{}}}
\bibcite{Scar04}{{63}{2009}{{Scarselli et~al.}}{{Scarselli, Gori, Tsoi, Hagenbuchner, and Monfardini}}}
\bibcite{schaeffer_double_2023}{{64}{2023}{{Schaeffer et~al.}}{{Schaeffer, Khona, Robertson, Boopathy, Pistunova, Rocks, Fiete, and Koyejo}}}
\bibcite{10.5555/2621980}{{65}{2014}{{Shalev-Shwartz and Ben-David}}{{}}}
\bibcite{sharma_bias-variance_2014}{{66}{2014}{{Sharma and Aiken}}{{}}}
\bibcite{shi2024homophilymodulatesdoubledescent}{{67}{2024}{{Shi et~al.}}{{Shi, Pan, Hu, and Dokmanić}}}
\bibcite{Sterkenburg_2024}{{68}{2024}{{Sterkenburg}}{{}}}
\bibcite{10.5555/2930837}{{69}{2015}{{Sugiyama}}{{}}}
\bibcite{tanis2024introductiongraphneuralnetworks}{{70}{2024}{{Tanis et~al.}}{{Tanis, Giannella, and Mariano}}}
\bibcite{10.1145/1968.1972}{{71}{1984}{{Valiant}}{{}}}
\bibcite{Vapnik1999-VAPTNO}{{72}{1999}{{Vapnik}}{{}}}
\bibcite{VapnikChervonenkis:1971}{{73}{1971}{{Vapnik and Chervonenkis}}{{}}}
\bibcite{Veli_kovi__2023}{{74}{2023}{{Veličković}}{{}}}
\bibcite{Wegener1987}{{75}{1987}{{Wegener}}{{}}}
\bibcite{XuRaginsky:2017:InfoGen}{{76}{2017}{{Xu and Raginsky}}{{}}}
\bibcite{XuMannor:2010:RobustnessGeneralization}{{77}{2010}{{Xu and Mannor}}{{}}}
\bibcite{yang_rethinking_2020}{{78}{2020}{{Yang et~al.}}{{Yang, Yu, You, Steinhardt, and Ma}}}
\bibcite{zhang_gradient_2019}{{79}{2019}{{Zhang}}{{}}}
\bibcite{zhang2023mathematical}{{80}{2023}{{Zhang}}{{}}}
\citation{6797087}
\citation{6797087}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proofs of theorems and results}{71}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Geman's derivation in \cite  {6797087}}{71}{subsection.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Theorem~\ref {thm:theorem_inconsistent}}{72}{subsection.A.2}\protected@file@percent }
\@writefile{loe}{\contentsline {col}{\ifthmt@listswap Corollary~A.2.1\else \numberline {A.2.1}Corollary\fi }{72}{col.1}\protected@file@percent }
\@writefile{loe}{\contentsline {col}{\ifthmt@listswap Corollary~A.2.2\else \numberline {A.2.2}Corollary\fi \thmtformatoptarg {Generalization bound - single hypothesis}}{73}{col.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Details for supervised learning setting}{73}{appendix.B}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces An illustration of the (supervised) statistical process. Phase III contains two parts: First is the evaluation $\nabla (h,c)$ according to the data $\mathcal  {D}$, and second is the $\mathsf  {Update}$ process to re-align $c$ to the actual target.}}{74}{figure.caption.28}\protected@file@percent }
\newlabel{fig:PhaseDiagram}{{23}{74}{An illustration of the (supervised) statistical process. Phase III contains two parts: First is the evaluation $\nabla (h,c)$ according to the data $\mathcal {D}$, and second is the $\mathsf {Update}$ process to re-align $c$ to the actual target}{figure.caption.28}{}}
\citation{10.5555/2930837,10.5555/200548}
\@writefile{toc}{\contentsline {section}{\numberline {C}About $\mathsf  {MLP}$ versus $\mathsf  {GNN}$}{76}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}Latent space}{77}{appendix.D}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {E}Idea for representation complexity}{77}{appendix.E}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.1}Structures of object}{78}{subsection.E.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {E.1.1}Representation}{79}{subsubsection.E.1.1}\protected@file@percent }
\@writefile{loe}{\contentsline {assume}{\ifthmt@listswap Assumption~E.1.1\else \numberline {E.1.1}Assumption\fi }{79}{assume.1}\protected@file@percent }
\@writefile{loe}{\contentsline {note}{\ifthmt@listswap Note~E.1\else \numberline {E.1}Note\fi }{80}{note.1}\protected@file@percent }
\citation{10.5555/2371238}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.2}Bias-variance intuition - No-free-lunch (\cite  {10.5555/2371238})}{82}{subsection.E.2}\protected@file@percent }
\@writefile{loe}{\contentsline {col}{\ifthmt@listswap Corollary~E.2.1\else \numberline {E.2.1}Corollary\fi }{82}{col.1}\protected@file@percent }
\newlabel{LastPage}{{E.2}{85}{Comment on statistical consideration}{page.85}{}}
\gdef\lastpage@lastpage{85}
\gdef\lastpage@lastpageHy{85}
\gdef \@abspage@last{85}
