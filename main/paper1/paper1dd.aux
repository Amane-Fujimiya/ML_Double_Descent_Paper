\relax 
\nicematrix@redefine@check@rerun 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{10.1145/1968.1972}
\citation{Vapnik1999-VAPTNO}
\citation{6797087}
\citation{belkin_reconciling_2019}
\citation{Vapnik1999-VAPTNO,10.5555/2371238,10.5555/2621980,STL_Hajek_Maxim_2021,bousquet2020theoryuniversallearning}
\citation{6797087,Domingos2000AUB}
\citation{belkin_reconciling_2019,schaeffer_double_2023,nakkiran_deep_2019,lafon_understanding_2024}
\citation{davies_unifying_2023,d_ascoli_triple_2020}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{belkin_reconciling_2019}
\citation{nakkiran_deep_2019}
\citation{Jacot:2018:NTK}
\citation{lafon_understanding_2024}
\citation{schaeffer_double_2023}
\citation{liu2023understandingroleoptimizationdouble}
\citation{davies_unifying_2023}
\citation{olmin2024understandingepochwisedoubledescent}
\citation{belkin_reconciling_2019}
\citation{nakkiran_deep_2019}
\citation{shi2024homophilymodulatesdoubledescent,schaeffer_double_2023,quetu_can_2023,lafon_understanding_2024,neal2019biasvariancetradeofftextbooksneed,davies_unifying_2023,quetu_can_2023-1,liu2023understandingroleoptimizationdouble,mei2020generalizationerrorrandomfeatures,mei2019generalization,adlam2020understandingdoubledescentrequires,transtrum2025egaddoubledescentexplained,liu2021kernelregressionhighdimensions,allerbo2025changingkerneltrainingleads,pezeshki2021multiscalefeaturelearningdynamics,brellmann2024on,nakkiran2019moredata,mei2019randomfeatures,heckel2020early,nakkiran2020regularization,Belkin_2020,Yang_2024,spiess2023doublesingledescentcausal,nakkiran2021optimalregularizationmitigatedouble,zhang2024manipulatingsparsedoubledescent,cherkassky2024understand,nakkiran2019datahurtlinearregression}
\citation{d_ascoli_triple_2020}
\citation{Sterkenburg_2024,Vapnik1999-VAPTNO,STL_Hajek_Maxim_2021}
\@writefile{toc}{\contentsline {section}{\numberline {2}Outline}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Work}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Problem statements}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Classical problem setting}{2}{subsection.4.1}\protected@file@percent }
\@writefile{loe}{\contentsline {assumption}{\ifthmt@listswap Assumption~4.1\else \numberline {4.1}Assumption\fi }{2}{assumption.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~4.1\else \numberline {4.1}Definition\fi \thmtformatoptarg {Empirical risk}}{3}{definition.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~4.2\else \numberline {4.2}Definition\fi \thmtformatoptarg {Generalization risk}}{3}{definition.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces {\relax \fontsize  {9}{10pt}\selectfont  \textbf  {Illustrative dynamic of the learning problem.} For an incremental hypothesis space sequence $\mathcal  {H}_{1},\mathcal  {H}_{2},\mathcal  {H}_{3}$, we aim to obtain the procedure $\mathcal  {T}$ that would either reach the \textit  {generalization solution} $\bm  {h}$, or the \textit  {empirical solution} $h^{*}$ for a particular hypothesis class, with respect to the concept $c$ and its observed concept $c'$. Model selection hence dictates how an algorithm or procedure than choose the best possible hypothesis to approximate the generalization solution from the empirical solution set. Do notice that here we explicitly state that the data would create a \textbf  {proxy concept} that overlaps with the concept class and of some arbitrary `distance' from the true concept by $d(c,c')$. In case the two classes overlap, there still exists the arbitrary distance. Also, we can also observe the intuitive notion of increasing hypothesis class - while it indeed can help in getting closer to the concept class, the somewhat intrinsic property of current learning theory being the relative random procedure class make it probabilistically unstable.}}}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:statlearnclassical}{{1}{3}{\small \textbf {Illustrative dynamic of the learning problem.} For an incremental hypothesis space sequence $\mathcal {H}_{1},\mathcal {H}_{2},\mathcal {H}_{3}$, we aim to obtain the procedure $\mathcal {T}$ that would either reach the \textit {generalization solution} $\bm {h}$, or the \textit {empirical solution} $h^{*}$ for a particular hypothesis class, with respect to the concept $c$ and its observed concept $c'$. Model selection hence dictates how an algorithm or procedure than choose the best possible hypothesis to approximate the generalization solution from the empirical solution set. Do notice that here we explicitly state that the data would create a \textbf {proxy concept} that overlaps with the concept class and of some arbitrary `distance' from the true concept by $d(c,c')$. In case the two classes overlap, there still exists the arbitrary distance. Also, we can also observe the intuitive notion of increasing hypothesis class - while it indeed can help in getting closer to the concept class, the somewhat intrinsic property of current learning theory being the relative random procedure class make it probabilistically unstable}{figure.caption.1}{}}
\citation{JMLR:v11:shalev-shwartz10a}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~4.3\else \numberline {4.3}Definition\fi \thmtformatoptarg {Empirical learning problem}}{4}{definition.3}\protected@file@percent }
\newlabel{eq:lp1}{{3}{4}{Empirical learning problem}{equation.3}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~4.4\else \numberline {4.4}Definition\fi \thmtformatoptarg {Generalization learning problem}}{4}{definition.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Uniform convergence}{4}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~4.5\else \numberline {4.5}Definition\fi }{4}{definition.5}\protected@file@percent }
\citation{achlioptas_stochastic_nodate,ruder_overview_2017}
\citation{zhang_gradient_2019}
\citation{gareth_james_introduction_2013}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~4.6\else \numberline {4.6}Definition\fi }{5}{definition.6}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~4.7\else \numberline {4.7}Definition\fi }{5}{definition.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Overfitting and underfitting}{5}{subsection.4.2}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~4.8\else \numberline {4.8}Definition\fi \thmtformatoptarg {Heuristic empirical risk}}{5}{definition.8}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~4.9\else \numberline {4.9}Definition\fi \thmtformatoptarg {Heuristical generalization risk}}{5}{definition.9}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~4.10\else \numberline {4.10}Definition\fi \thmtformatoptarg {Underfitting}}{5}{definition.10}\protected@file@percent }
\citation{gareth_james_introduction_2013,goodfellow2016deep,STL_Hajek_Maxim_2021,10.5555/2371238,10.5555/2621980}
\citation{LehmannCasella_theory_1998,liam_statistics_2005}
\citation{6797087}
\citation{6797087}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~4.11\else \numberline {4.11}Definition\fi \thmtformatoptarg {Overfitting}}{6}{definition.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Bias-variance tradeoff}{6}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Precursor (Geman et al., 1992)}{6}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~4.2\else \numberline {4.2}Theorem\fi \thmtformatoptarg {Bias-variance decomposition}}{6}{theorem.2}\protected@file@percent }
\citation{brown2024biasvariance}
\citation{adlam2020understandingdoubledescentrequires}
\citation{brown2024biasvariance,PfauBregmanDivergence}
\citation{lafon_understanding_2024}
\citation{6797087,sharma_bias-variance_2014,domingos_unifeid_2000,adlam2020understandingdoubledescentrequires,yang_rethinking_2020}
\citation{adlam2020understandingdoubledescentrequires}
\newlabel{SC@1}{{\caption@xref {??}{ on input line 235}}{7}{Precursor (Geman et al., 1992)}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Decomposition of the error term into 3 parts}. Respectively, the irreducible error $\mathbb  {V}[y(\mathbf  {x})]$, the variance (blue) and the bias (dark yellow). All of them are assumed to take up 100\% of the error observed in such composition. The proportion is included using the three coefficient $\lambda _{\epsilon },\lambda _{V},\lambda _{B}$ where $\lambda _{\epsilon }+\lambda _{V}+\lambda _{B}=1$.}}{7}{figure.caption.2}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~4.3\else \numberline {4.3}Theorem\fi \thmtformatoptarg {Bias-variance tradeoff}}{7}{theorem.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Expansion and variations}{7}{subsubsection.4.3.2}\protected@file@percent }
\citation{domingos_unified_aaai_2000,domingos_unified_2000}
\citation{10.5555/2371238,lafon_understanding_2024}
\citation{brown2024biasvariance}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Generalized decomposition}{8}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Approximation-Estimation tradeoff}{8}{subsubsection.4.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Double descent}{8}{subsection.4.4}\protected@file@percent }
\citation{belkin_reconciling_2019}
\citation{belkin_reconciling_2019}
\citation{belkin_reconciling_2019}
\citation{belkin_reconciling_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces {\bf  Curves for training risk (dashed line) and test risk (solid line).} ({\bf  a}) The classical \emph  {U-shaped risk curve} arising from the bias-variance trade-off. ({\bf  b}) The \emph  {double descent risk curve}, which incorporates the U-shaped risk curve (i.e., the ``classical'' regime) together with the observed behaviour from using high capacity function classes (i.e., the ``modern'' interpolating regime), separated by the interpolation threshold. The predictors to the right of the interpolation threshold have zero training risk. Reproduced from \cite  {belkin_reconciling_2019}.}}{9}{figure.caption.3}\protected@file@percent }
\newlabel{fig:double-descent}{{3}{9}{{\bf Curves for training risk (dashed line) and test risk (solid line).} ({\bf a}) The classical \emph {U-shaped risk curve} arising from the bias-variance trade-off. ({\bf b}) The \emph {double descent risk curve}, which incorporates the U-shaped risk curve (i.e., the ``classical'' regime) together with the observed behaviour from using high capacity function classes (i.e., the ``modern'' interpolating regime), separated by the interpolation threshold. The predictors to the right of the interpolation threshold have zero training risk. Reproduced from \cite {belkin_reconciling_2019}}{figure.caption.3}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~4.12\else \numberline {4.12}Definition\fi \thmtformatoptarg {Interpolation threshold}}{9}{definition.12}\protected@file@percent }
\citation{10.5555/2371238,10.5555/2930837,10.5555/2621980}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\bf  Left:} Train and test error as a function of model size, for ResNet18s of varying width on CIFAR-10 with 15\% label noise. {\bf  Right:} Test error, shown for varying train epochs. All models trained using Adam for 4K epochs. The largest model (width $64$) corresponds to standard ResNet18. Resued from \cite  {nakkiran_deep_2019}. }}{10}{figure.caption.4}\protected@file@percent }
\newlabel{fig:errorvscomplexity}{{4}{10}{{\bf Left:} Train and test error as a function of model size, for ResNet18s of varying width on CIFAR-10 with 15\% label noise. {\bf Right:} Test error, shown for varying train epochs. All models trained using Adam for 4K epochs. The largest model (width $64$) corresponds to standard ResNet18. Resued from \cite {nakkiran_deep_2019}}{figure.caption.4}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~4.13\else \numberline {4.13}Definition\fi \thmtformatoptarg {Effective Model Complexity}}{10}{definition.13}\protected@file@percent }
\@writefile{loe}{\contentsline {hypothesis}{\ifthmt@listswap Hypothesis~4.1\else \numberline {4.1}Hypothesis\fi \thmtformatoptarg {Generalized Double Descent hypothesis, informal}}{10}{hypothesis.1}\protected@file@percent }
\newlabel{hyp:informaldd}{{4.1}{10}{Generalized Double Descent hypothesis, informal}{hypothesis.1}{}}
\citation{nakkiran_deep_2019}
\citation{belkin_reconciling_2019}
\citation{advani2017highdimensionaldynamicsgeneralizationerror}
\citation{belkin2018understanddeeplearningneed}
\citation{mei2020generalizationerrorrandomfeatures}
\citation{10.5555/2371238}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces {\bf  Left:} Test error as a function of model size and train epochs. The horizontal line corresponds to model-wise double descent--varying model size while training for as long as possible. The vertical line corresponds to epoch-wise double descent, with test error undergoing double-descent as train time increases. {\bf  Right} Train error of the corresponding models. All models are Resnet18s trained on CIFAR-10 with 15\% label noise, data-augmentation, and Adam for up to 4K epochs. Reused from \cite  {nakkiran_deep_2019}}}{11}{figure.caption.5}\protected@file@percent }
\newlabel{fig:unified}{{5}{11}{{\bf Left:} Test error as a function of model size and train epochs. The horizontal line corresponds to model-wise double descent--varying model size while training for as long as possible. The vertical line corresponds to epoch-wise double descent, with test error undergoing double-descent as train time increases. {\bf Right} Train error of the corresponding models. All models are Resnet18s trained on CIFAR-10 with 15\% label noise, data-augmentation, and Adam for up to 4K epochs. Reused from \cite {nakkiran_deep_2019}}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}No-free-lunch}{11}{section.5}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~5.1\else \numberline {5.1}Theorem\fi \thmtformatoptarg {\cite  {10.5555/2371238}}}{11}{theorem.1}\protected@file@percent }
\newlabel{thm:minimalNeu}{{5.1}{11}{\cite {10.5555/2371238}}{theorem.1}{}}
\citation{belkin_reconciling_2019}
\citation{nakkiran_deep_2019}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~5.2\else \numberline {5.2}Theorem\fi \thmtformatoptarg {No-Free-Lunch}}{12}{theorem.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Sensitivity of hypothesis}{12}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Previous attempts}{12}{section.6}\protected@file@percent }
\citation{belkin_reconciling_2019}
\citation{nakkiran_deep_2019}
\citation{Yang_2024,nakkiran2019moredata,schaeffer_double_2023,lafon_understanding_2024}
\citation{NEURIPS2024_2d43f7a6}
\citation{liu2021kernelregressionhighdimensions}
\citation{zhang2024manipulatingsparsedoubledescent,allerbo2025changingkerneltrainingleads}
\citation{brellmann2024on}
\citation{harzli2023doubledescentcurvesneuralnetworks}
\citation{belkin_reconciling_2019}
\citation{luo2023doubledescentdiscrepancytask,Yang_2024,pezeshki2021multiscalefeaturelearningdynamics}
\citation{nakkiran_deep_2019}
\citation{shi2024homophilymodulatesdoubledescent}
\citation{schaeffer_double_2023,ErdongHu2021}
\citation{shi2024homophilymodulatesdoubledescent}
\citation{cherkassky2024understand}
\citation{lee2022vctheoreticalexplanationdouble}
\citation{hastie2019surprises}
\citation{allerbo2025changingkerneltrainingleads}
\citation{modelcomplex_exp,hu_model_2021,janik2021complexitydeepneuralnetworks,truong2025rademachercomplexitybasedgeneralizationbounds,luo2024investigatingimpactmodelcomplexity}
\citation{neal2018modern}
\@writefile{loe}{\contentsline {conjecture}{\ifthmt@listswap Conjecture~6.1\else \numberline {6.1}Conjecture\fi }{13}{conjecture.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Discovery of double descent}{13}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Theoretical attacks}{13}{subsection.6.2}\protected@file@percent }
\citation{power2022grokkinggeneralizationoverfittingsmall}
\citation{wei2022emergentabilitieslargelanguage}
\citation{soudry2024implicitbiasgradientdescent}
\citation{kaplan2020scalinglawsneurallanguage,wei2022emergentabilitieslargelanguage}
\citation{vandeven2024continuallearningcatastrophicforgetting}
\citation{Manin_2024}
\citation{hu2021modelcomplexitydeeplearning,luo2024investigatingimpactmodelcomplexity,barceló2020modelinterpretabilitylenscomputational,Molnar_2020,janik2021complexitydeepneuralnetworks}
\citation{hu2021modelcomplexitydeeplearning}
\citation{Molnar_2020}
\citation{belkin_reconciling_2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Problems}{14}{subsection.6.3}\protected@file@percent }
\citation{nakkiran_deep_2019}
\citation{10.5555/2371238}
\citation{electronics13020416}
\citation{lipton2016mythos,doshi2017towards,molnar2020general}
\citation{molnar2020general}
\citation{dreyfus1965alchemy,dreyfus1972what,dreyfus1986mind,suchman1987plans,brooks1991intelligence,searle1980minds,mccarthy1969philosophical,harnad1990symbol}
\citation{pearl2009causality,marcus2018deep,sutton2019bitter}
\citation{lipton2018troublingtrendsmachinelearning}
\citation{romer2015mathiness,syll2024postreal}
\citation{kapoor2022leakage}
\citation{Jacot:2018:NTK}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Bounding inequalities}{16}{subsection.6.4}\protected@file@percent }
\citation{zhang2017understandingdeeplearningrequires}
\citation{truong2025rademachercomplexitybasedgeneralizationbounds}
\citation{nagarajan2021uniformconvergenceunableexplain}
\citation{tunali2019empirical}
\citation{bartlett2005local,bartlett2017spectrally,bousquet2002stability,neyshabur2015norm}
\citation{10.5555/200548}
\citation{Vapnik1999-VAPTNO}
\@writefile{loe}{\contentsline {proposition}{\ifthmt@listswap Proposition~6.1\else \numberline {6.1}Proposition\fi }{17}{proposition.1}\protected@file@percent }
\newlabel{prop:PAC_ERM_1_recite}{{6.1}{17}{}{proposition.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Complexity notions}{17}{subsection.6.5}\protected@file@percent }
\citation{STL_Hajek_Maxim_2021,10.5555/2371238,10.5555/2621980}
\citation{10.5555/200548}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Descriptions of complexity measures applied to models and learning processes}}{19}{table.caption.7}\protected@file@percent }
\newlabel{tab:complexity-measures}{{1}{19}{Descriptions of complexity measures applied to models and learning processes}{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.1}Representation complexity}{19}{subsubsection.6.5.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~6.1\else \numberline {6.1}Definition\fi \thmtformatoptarg {Representation scheme}}{19}{definition.1}\protected@file@percent }
\citation{Wegener1987,MiltersenRadhakrishnanWegener2005,DarwicheMarquis2002}
\citation{huang2025samplecomplexityrepresentationability,wegel2025samplecomplexitysemisupervisedmultiobjective,Mei2022TowardsBridging}
\citation{Balcan2010TrueSampleComplexity}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~6.2\else \numberline {6.2}Definition\fi \thmtformatoptarg {Representation complexity}}{20}{definition.2}\protected@file@percent }
\@writefile{loe}{\contentsline {assumption}{\ifthmt@listswap Assumption~6.2\else \numberline {6.2}Assumption\fi }{20}{assumption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2}Optimization complexity}{20}{subsubsection.6.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.3}Sample complexity}{20}{subsubsection.6.5.3}\protected@file@percent }
\citation{lee2022vctheoreticalexplanationdouble}
\citation{10.5555/2621980,10.5555/2371238,STL_Hajek_Maxim_2021}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~6.3\else \numberline {6.3}Definition\fi \thmtformatoptarg {Sample complexity}}{21}{definition.3}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~6.4\else \numberline {6.4}Definition\fi \thmtformatoptarg {$(\epsilon ,\mu )$-sample complexity}}{21}{definition.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.4}Structural mass}{21}{subsubsection.6.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Classical insight}{21}{subsection.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1}PAC-learning setup}{21}{subsubsection.6.6.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~6.5\else \numberline {6.5}Definition\fi \thmtformatoptarg {PAC-learning}}{21}{definition.5}\protected@file@percent }
\citation{10.5555/2371238}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~6.6\else \numberline {6.6}Definition\fi \thmtformatoptarg {Consistent Learner}}{22}{definition.6}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~6.3\else \numberline {6.3}Theorem\fi \thmtformatoptarg {Learning bound - finite $\mathcal  {H}$, consistent case}}{22}{theorem.3}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~6.4\else \numberline {6.4}Theorem\fi \thmtformatoptarg {Learning bound - finite $\mathcal  {H}$, inconsistent case}}{22}{theorem.4}\protected@file@percent }
\newlabel{thm:theorem_inconsistent}{{6.4}{22}{Learning bound - finite $\mathcal {H}$, inconsistent case}{theorem.4}{}}
\citation{lafon_understanding_2024}
\@writefile{toc}{\contentsline {section}{\numberline {7}Analysis}{23}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Linear function class}{23}{subsection.7.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~7.1\else \numberline {7.1}Definition\fi }{23}{definition.1}\protected@file@percent }
\newlabel{eq:linear_gaussian_erm}{{27}{23}{Linear function class}{equation.27}{}}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~7.1\else \numberline {7.1}Theorem\fi \thmtformatoptarg {Double descent on linear regression}}{23}{theorem.1}\protected@file@percent }
\newlabel{thm:double_descent_lr}{{7.1}{23}{Double descent on linear regression}{theorem.1}{}}
\citation{lafon_understanding_2024}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Theorem~\ref {thm:double_descent_lr} behaviours on randomized setting. For this model, we have $d=100$, $\sigma = 0.5$, and variational $n$. Full test cases are for $p=[1,100]$, $n=\{12,24,36,50,76,82,90,100,106,112,121\}$ accordingly. The test function of the concept itself is the same linear model, but with different input only.}}{24}{figure.caption.8}\protected@file@percent }
\newlabel{fig:theorem_double_descent_fig}{{6}{24}{Theorem~\ref {thm:double_descent_lr} behaviours on randomized setting. For this model, we have $d=100$, $\sigma = 0.5$, and variational $n$. Full test cases are for $p=[1,100]$, $n=\{12,24,36,50,76,82,90,100,106,112,121\}$ accordingly. The test function of the concept itself is the same linear model, but with different input only}{figure.caption.8}{}}
\citation{nakkiran2019datahurtlinearregression}
\citation{goodfellow2016deep,10.5555/2930837}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Contrary to Theorem~\ref {thm:double_descent_lr}, this is the behaviours on randomized setting for standard parameter-wised mean square error measure (MSE-on-parameter). For this model, we have $d=100$, $\sigma = 0.5$, and variational $n$. Full test cases are for $p=[1,100]$, $n=\{12,24,36,50,76,82,90,100,106,112,121\}$ accordingly. The test function of the concept itself is the same linear model, but with different input only.}}{25}{figure.caption.9}\protected@file@percent }
\newlabel{fig:theorem_double_descent_fig2}{{7}{25}{Contrary to Theorem~\ref {thm:double_descent_lr}, this is the behaviours on randomized setting for standard parameter-wised mean square error measure (MSE-on-parameter). For this model, we have $d=100$, $\sigma = 0.5$, and variational $n$. Full test cases are for $p=[1,100]$, $n=\{12,24,36,50,76,82,90,100,106,112,121\}$ accordingly. The test function of the concept itself is the same linear model, but with different input only}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Linear function class extension}{25}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Polynomials}{25}{subsection.7.3}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~7.2\else \numberline {7.2}Definition\fi \thmtformatoptarg {Univariate polynomial}}{25}{definition.2}\protected@file@percent }
\citation{McArtneyInterpolation2003}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Contrary to Theorem~\ref {thm:double_descent_lr} and both figure~\ref {fig:theorem_double_descent_fig2}, this is the behaviours on randomized setting for standard parameter-wised mean square error measure (MSE-on-prediction). Especially, we do not see double descent pattern in this setting, as it is. For this model, we have $d=100$, $\sigma = 0.5$, and variational $n$. Full test cases are for $p=[1,100]$, $n=\{12,24,36,50,76,82,90,100,106,112,121\}$ accordingly. The test function of the concept itself is the same linear model, but with different input only.}}{26}{figure.caption.10}\protected@file@percent }
\newlabel{fig:theorem_double_descent_fig3}{{8}{26}{Contrary to Theorem~\ref {thm:double_descent_lr} and both figure~\ref {fig:theorem_double_descent_fig2}, this is the behaviours on randomized setting for standard parameter-wised mean square error measure (MSE-on-prediction). Especially, we do not see double descent pattern in this setting, as it is. For this model, we have $d=100$, $\sigma = 0.5$, and variational $n$. Full test cases are for $p=[1,100]$, $n=\{12,24,36,50,76,82,90,100,106,112,121\}$ accordingly. The test function of the concept itself is the same linear model, but with different input only}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Similar experiment according to setting and result of Theorem~\ref {eq:linear_gaussian_erm}, where $p$ is in the range [1,5523]. Here, we can see small double descent modelled exactly at the interpolation threshold, while the later region exhibit similar phenomenon as bias-variance tradeoff.}}{26}{figure.caption.11}\protected@file@percent }
\newlabel{fig:contrarian}{{9}{26}{Similar experiment according to setting and result of Theorem~\ref {eq:linear_gaussian_erm}, where $p$ is in the range [1,5523]. Here, we can see small double descent modelled exactly at the interpolation threshold, while the later region exhibit similar phenomenon as bias-variance tradeoff}{figure.caption.11}{}}
\newlabel{eq:soft_hard_interpolation}{{35}{27}{Polynomials}{equation.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Synthetic double descent curve illustration, with 3 axes of dependency supposed of the parameterization case - the axis of the number of data point in soft interpolation, the saddle points given of the model, and the overall error observed of the model. We did not take into account the relative curve of the test error.}}{27}{figure.caption.12}\protected@file@percent }
\newlabel{fig:descent_synthetic_curve}{{10}{27}{Synthetic double descent curve illustration, with 3 axes of dependency supposed of the parameterization case - the axis of the number of data point in soft interpolation, the saddle points given of the model, and the overall error observed of the model. We did not take into account the relative curve of the test error}{figure.caption.12}{}}
\newlabel{eq:soft_hard_interpolation_role}{{36}{28}{Polynomials}{equation.36}{}}
\@writefile{loe}{\contentsline {hypothesis}{\ifthmt@listswap Hypothesis~7.1\else \numberline {7.1}Hypothesis\fi \thmtformatoptarg {Polynomial double descent}}{28}{hypothesis.1}\protected@file@percent }
\citation{arjevani2025geometryoptimizationshallowpolynomial}
\@writefile{loe}{\contentsline {conjecture}{\ifthmt@listswap Conjecture~7.1\else \numberline {7.1}Conjecture\fi \thmtformatoptarg {Double descent}}{29}{conjecture.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Polynomial model structure}{29}{subsubsection.7.3.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~7.3\else \numberline {7.3}Definition\fi \thmtformatoptarg {Shallow polynomial neural network}}{29}{definition.3}\protected@file@percent }
\newlabel{def:poly_structure1}{{7.3}{29}{Shallow polynomial neural network}{definition.3}{}}
\newlabel{eq:network_model}{{37}{29}{Shallow polynomial neural network}{equation.37}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~7.4\else \numberline {7.4}Definition\fi \thmtformatoptarg {Polynomial neural network, testing}}{29}{definition.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Testing result}{29}{subsubsection.7.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Controllable parameters and hyperparameters for polynomial class analysis}}{30}{table.caption.14}\protected@file@percent }
\newlabel{tab:polynomial_test}{{2}{30}{Controllable parameters and hyperparameters for polynomial class analysis}{table.caption.14}{}}
\citation{goodfellow2016deep,zhang2023mathematical}
\citation{zhang2023divedeeplearning}
\citation{10.5555/2721661}
\citation{10.5555/2721661}
\@writefile{toc}{\contentsline {section}{\numberline {8}Axiomatic formalization}{31}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Neural network formalism}{31}{subsection.8.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.1\else \numberline {8.1}Definition\fi \thmtformatoptarg {Standard multilayer network, \cite  {zhang2023divedeeplearning}}}{31}{definition.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Flow diagram of the shallow neural network structure in linear and polynomial case, following the notation of \cite  {10.5555/2721661} in \textit  {Neural Network Design}. The Support Vector Machine flow diagram looks similar to the case of polynomial, except a decision function at the end of the output, and changing the function $f$ into the kernel function $K(x',x)$.}}{31}{figure.caption.15}\protected@file@percent }
\citation{chen2021equivalence,wang2019svmdsn,zhang2014equivalence}
\citation{domingos_unifeid_2000}
\citation{brown2024biasvariance}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~8.2\else \numberline {8.2}Definition\fi \thmtformatoptarg {Support vector machine setting}}{32}{definition.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Hypothesis}{32}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Stability of bias-variance measure}{32}{subsubsection.8.2.1}\protected@file@percent }
\citation{lafon_understanding_2024}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Alternative measures}{33}{subsubsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3}Inductive bias}{33}{subsubsection.8.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.4}Randomness and probabilistic setting}{33}{subsubsection.8.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.5}Knowledge masking}{33}{subsubsection.8.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.6}Intrinsic Data Factors}{33}{subsubsection.8.2.6}\protected@file@percent }
\bibdata{references}
\bibcite{achlioptas_stochastic_nodate}{{1}{}{{Achlioptas}}{{}}}
\bibcite{adlam2020understandingdoubledescentrequires}{{2}{2020}{{Adlam \& Pennington}}{{Adlam and Pennington}}}
\bibcite{advani2017highdimensionaldynamicsgeneralizationerror}{{3}{2017}{{Advani \& Saxe}}{{Advani and Saxe}}}
\bibcite{allerbo2025changingkerneltrainingleads}{{4}{2025}{{Allerbo}}{{}}}
\bibcite{arjevani2025geometryoptimizationshallowpolynomial}{{5}{2025}{{Arjevani et~al.}}{{Arjevani, Bruna, Kileel, Polak, and Trager}}}
\bibcite{Balcan2010TrueSampleComplexity}{{6}{2010}{{Balcan et~al.}}{{Balcan, Hanneke, and Wortman~Vaughan}}}
\bibcite{electronics13020416}{{7}{2024}{{Barbierato \& Gatti}}{{Barbierato and Gatti}}}
\bibcite{barceló2020modelinterpretabilitylenscomputational}{{8}{2020}{{Barceló et~al.}}{{Barceló, Monet, Pérez, and Subercaseaux}}}
\bibcite{bartlett2005local}{{9}{2005}{{Bartlett \& Mendelson}}{{Bartlett and Mendelson}}}
\bibcite{bartlett2017spectrally}{{10}{2017}{{Bartlett et~al.}}{{Bartlett, Foster, and Telgarsky}}}
\bibcite{belkin2018understanddeeplearningneed}{{11}{2018}{{Belkin et~al.}}{{Belkin, Ma, and Mandal}}}
\bibcite{belkin_reconciling_2019}{{12}{2019}{{Belkin et~al.}}{{Belkin, Hsu, Ma, and Mandal}}}
\bibcite{Belkin_2020}{{13}{2020}{{Belkin et~al.}}{{Belkin, Hsu, and Xu}}}
\bibcite{bousquet2002stability}{{14}{2002}{{Bousquet \& Elisseeff}}{{Bousquet and Elisseeff}}}
\bibcite{bousquet2020theoryuniversallearning}{{15}{2020}{{Bousquet et~al.}}{{Bousquet, Hanneke, Moran, van Handel, and Yehudayoff}}}
\bibcite{brellmann2024on}{{16}{2024}{{Brellmann et~al.}}{{Brellmann, Berthier, Filliat, and Frehse}}}
\bibcite{brooks1991intelligence}{{17}{1991}{{Brooks}}{{}}}
\bibcite{brown2024biasvariance}{{18}{2024}{{Brown \& Ali}}{{Brown and Ali}}}
\bibcite{chen2021equivalence}{{19}{2021}{{Chen et~al.}}{{Chen, Huang, Nguyen, and Weng}}}
\bibcite{cherkassky2024understand}{{20}{2024}{{Cherkassky \& Lee}}{{Cherkassky and Lee}}}
\bibcite{d_ascoli_triple_2020}{{21}{2020}{{d'~Ascoli et~al.}}{{d'~Ascoli, Sagun, and Biroli}}}
\bibcite{DarwicheMarquis2002}{{22}{2002}{{Darwiche \& Marquis}}{{Darwiche and Marquis}}}
\bibcite{davies_unifying_2023}{{23}{2023}{{Davies et~al.}}{{Davies, Langosco, and Krueger}}}
\bibcite{10.5555/2721661}{{24}{2014}{{Demuth et~al.}}{{Demuth, Beale, De~Jess, and Hagan}}}
\bibcite{domingos_unified_2000}{{25}{2000{a}}{{Domingos}}{{}}}
\bibcite{domingos_unified_aaai_2000}{{26}{2000{b}}{{Domingos}}{{}}}
\bibcite{Domingos2000AUB}{{27}{2000{c}}{{Domingos}}{{}}}
\bibcite{domingos_unifeid_2000}{{28}{2000{d}}{{Domingos}}{{}}}
\bibcite{doshi2017towards}{{29}{2017}{{Doshi-Velez \& Kim}}{{Doshi-Velez and Kim}}}
\bibcite{dreyfus1965alchemy}{{30}{1965}{{Dreyfus}}{{}}}
\bibcite{dreyfus1972what}{{31}{1972}{{Dreyfus}}{{}}}
\bibcite{dreyfus1986mind}{{32}{1986}{{Dreyfus \& Dreyfus}}{{Dreyfus and Dreyfus}}}
\bibcite{LehmannCasella_theory_1998}{{33}{1998}{{E.~L.~Lehmann}}{{}}}
\bibcite{modelcomplex_exp}{{34}{}{{et~al.}}{{}}}
\bibcite{6797087}{{35}{1992}{{Geman et~al.}}{{Geman, Bienenstock, and Doursat}}}
\bibcite{goodfellow2016deep}{{36}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{STL_Hajek_Maxim_2021}{{37}{2021}{{Hajek \& Raginsky}}{{Hajek and Raginsky}}}
\bibcite{harnad1990symbol}{{38}{1990}{{Harnad}}{{}}}
\bibcite{harzli2023doubledescentcurvesneuralnetworks}{{39}{2023}{{Harzli et~al.}}{{Harzli, Grau, Valle-Pérez, and Louis}}}
\bibcite{hastie2019surprises}{{40}{2019}{{Hastie et~al.}}{{Hastie, Montanari, Rosset, and Tibshirani}}}
\bibcite{heckel2020early}{{41}{2020}{{Heckel \& Yilmaz}}{{Heckel and Yilmaz}}}
\bibcite{ErdongHu2021}{{42}{2021}{{Hu}}{{}}}
\bibcite{hu2021modelcomplexitydeeplearning}{{43}{2021{a}}{{Hu et~al.}}{{Hu, Chu, Pei, Liu, and Bian}}}
\bibcite{hu_model_2021}{{44}{2021{b}}{{Hu et~al.}}{{Hu, Chu, Pei, Liu, and Bian}}}
\bibcite{huang2025samplecomplexityrepresentationability}{{45}{2025}{{Huang et~al.}}{{Huang, Li, Wu, Yang, Talwalkar, Ramchandran, Jordan, and Jiao}}}
\bibcite{Jacot:2018:NTK}{{46}{2018}{{Jacot et~al.}}{{Jacot, Gabriel, and Hongler}}}
\bibcite{gareth_james_introduction_2013}{{47}{2013}{{James et~al.}}{{James, Hastie, Tibshirani, and Witten}}}
\bibcite{janik2021complexitydeepneuralnetworks}{{48}{2021}{{Janik \& Witaszczyk}}{{Janik and Witaszczyk}}}
\bibcite{kaplan2020scalinglawsneurallanguage}{{49}{2020}{{Kaplan et~al.}}{{Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}}}
\bibcite{kapoor2022leakage}{{50}{2022}{{Kapoor \& Narayanan}}{{Kapoor and Narayanan}}}
\bibcite{10.5555/200548}{{51}{1994}{{Kearns \& Vazirani}}{{Kearns and Vazirani}}}
\bibcite{lafon_understanding_2024}{{52}{2024}{{Lafon \& Thomas}}{{Lafon and Thomas}}}
\bibcite{lee2022vctheoreticalexplanationdouble}{{53}{2022}{{Lee \& Cherkassky}}{{Lee and Cherkassky}}}
\bibcite{NEURIPS2024_2d43f7a6}{{54}{2024}{{Li \& Sonthalia}}{{Li and Sonthalia}}}
\bibcite{lipton2016mythos}{{55}{2018}{{Lipton}}{{}}}
\bibcite{lipton2018troublingtrendsmachinelearning}{{56}{2018}{{Lipton \& Steinhardt}}{{Lipton and Steinhardt}}}
\bibcite{liu2023understandingroleoptimizationdouble}{{57}{2023}{{Liu \& Flanigan}}{{Liu and Flanigan}}}
\bibcite{liu2021kernelregressionhighdimensions}{{58}{2021}{{Liu et~al.}}{{Liu, Liao, and Suykens}}}
\bibcite{luo2024investigatingimpactmodelcomplexity}{{59}{2024}{{Luo et~al.}}{{Luo, Wang, and Huang}}}
\bibcite{luo2023doubledescentdiscrepancytask}{{60}{2023}{{Luo \& Dong}}{{Luo and Dong}}}
\bibcite{Manin_2024}{{61}{2024}{{Manin \& Marcolli}}{{Manin and Marcolli}}}
\bibcite{marcus2018deep}{{62}{2018}{{Marcus}}{{}}}
\bibcite{mccarthy1969philosophical}{{63}{1969}{{McCarthy \& Hayes}}{{McCarthy and Hayes}}}
\bibcite{Mei2022TowardsBridging}{{64}{2022}{{Mei et~al.}}{{Mei, Zhao, Yuan, and Ni}}}
\bibcite{mei2019generalization}{{65}{2019{a}}{{Mei \& Montanari}}{{Mei and Montanari}}}
\bibcite{mei2019randomfeatures}{{66}{2019{b}}{{Mei \& Montanari}}{{Mei and Montanari}}}
\bibcite{mei2020generalizationerrorrandomfeatures}{{67}{2020}{{Mei \& Montanari}}{{Mei and Montanari}}}
\bibcite{MiltersenRadhakrishnanWegener2005}{{68}{2005}{{Miltersen et~al.}}{{Miltersen, Radhakrishnan, and Wegener}}}
\bibcite{10.5555/2371238}{{69}{2012}{{Mohri et~al.}}{{Mohri, Rostamizadeh, and Talwalkar}}}
\bibcite{Molnar_2020}{{70}{2020{a}}{{Molnar et~al.}}{{Molnar, Casalicchio, and Bischl}}}
\bibcite{molnar2020general}{{71}{2020{b}}{{Molnar et~al.}}{{Molnar, König, Herbinger, Freiesleben, Dandl, Scholbeck, Casalicchio, Grosse-Wentrup, and Bischl}}}
\bibcite{nagarajan2021uniformconvergenceunableexplain}{{72}{2021}{{Nagarajan \& Kolter}}{{Nagarajan and Kolter}}}
\bibcite{nakkiran2019datahurtlinearregression}{{73}{2019{a}}{{Nakkiran}}{{}}}
\bibcite{nakkiran2019moredata}{{74}{2019{b}}{{Nakkiran}}{{}}}
\bibcite{nakkiran_deep_2019}{{75}{2019}{{Nakkiran et~al.}}{{Nakkiran, Kaplun, Bansal, Yang, Barak, and Sutskever}}}
\bibcite{nakkiran2020regularization}{{76}{2020}{{Nakkiran et~al.}}{{Nakkiran, Venkat, Kakade, and Ma}}}
\bibcite{nakkiran2021optimalregularizationmitigatedouble}{{77}{2021}{{Nakkiran et~al.}}{{Nakkiran, Venkat, Kakade, and Ma}}}
\bibcite{neal2019biasvariancetradeofftextbooksneed}{{78}{2019}{{Neal}}{{}}}
\bibcite{neal2018modern}{{79}{2018}{{Neal et~al.}}{{Neal, Mittal, Baratin, Tantia, Scicluna, Lacoste-Julien, and Mitliagkas}}}
\bibcite{neyshabur2015norm}{{80}{2015}{{Neyshabur et~al.}}{{Neyshabur, Tomioka, and Srebro}}}
\bibcite{olmin2024understandingepochwisedoubledescent}{{81}{2024}{{Olmin \& Lindsten}}{{Olmin and Lindsten}}}
\bibcite{liam_statistics_2005}{{82}{2005}{{Paninski}}{{}}}
\bibcite{pearl2009causality}{{83}{2009}{{Pearl}}{{}}}
\bibcite{pezeshki2021multiscalefeaturelearningdynamics}{{84}{2021}{{Pezeshki et~al.}}{{Pezeshki, Mitra, Bengio, and Lajoie}}}
\bibcite{PfauBregmanDivergence}{{85}{2013}{{Pfau}}{{}}}
\bibcite{McArtneyInterpolation2003}{{86}{2003}{{Phillips}}{{}}}
\bibcite{power2022grokkinggeneralizationoverfittingsmall}{{87}{2022}{{Power et~al.}}{{Power, Burda, Edwards, Babuschkin, and Misra}}}
\bibcite{quetu_can_2023}{{88}{2023{a}}{{Quétu \& Tartaglione}}{{Quétu and Tartaglione}}}
\bibcite{quetu_can_2023-1}{{89}{2023{b}}{{Quétu \& Tartaglione}}{{Quétu and Tartaglione}}}
\bibcite{romer2015mathiness}{{90}{2015}{{Romer}}{{}}}
\bibcite{ruder_overview_2017}{{91}{2017}{{Ruder}}{{}}}
\bibcite{schaeffer_double_2023}{{92}{2023}{{Schaeffer et~al.}}{{Schaeffer, Khona, Robertson, Boopathy, Pistunova, Rocks, Fiete, and Koyejo}}}
\bibcite{searle1980minds}{{93}{1980}{{Searle}}{{}}}
\bibcite{10.5555/2621980}{{94}{2014}{{Shalev-Shwartz \& Ben-David}}{{Shalev-Shwartz and Ben-David}}}
\bibcite{JMLR:v11:shalev-shwartz10a}{{95}{2010}{{Shalev-Shwartz et~al.}}{{Shalev-Shwartz, Shamir, Srebro, and Sridharan}}}
\bibcite{sharma_bias-variance_2014}{{96}{2014}{{Sharma \& Aiken}}{{Sharma and Aiken}}}
\bibcite{shi2024homophilymodulatesdoubledescent}{{97}{2024}{{Shi et~al.}}{{Shi, Pan, Hu, and Dokmanić}}}
\bibcite{soudry2024implicitbiasgradientdescent}{{98}{2024}{{Soudry et~al.}}{{Soudry, Hoffer, Nacson, Gunasekar, and Srebro}}}
\bibcite{spiess2023doublesingledescentcausal}{{99}{2023}{{Spiess et~al.}}{{Spiess, Imbens, and Venugopal}}}
\bibcite{Sterkenburg_2024}{{100}{2024}{{Sterkenburg}}{{}}}
\bibcite{suchman1987plans}{{101}{1987}{{Suchman}}{{}}}
\bibcite{10.5555/2930837}{{102}{2015}{{Sugiyama}}{{}}}
\bibcite{sutton2019bitter}{{103}{2019}{{Sutton}}{{}}}
\bibcite{syll2024postreal}{{104}{2024}{{Syll}}{{}}}
\bibcite{transtrum2025egaddoubledescentexplained}{{105}{2025}{{Transtrum et~al.}}{{Transtrum, Hart, Jarvis, and Whitehead}}}
\bibcite{truong2025rademachercomplexitybasedgeneralizationbounds}{{106}{2025}{{Truong}}{{}}}
\bibcite{tunali2019empirical}{{107}{2019}{{Tunali}}{{}}}
\bibcite{10.1145/1968.1972}{{108}{1984}{{Valiant}}{{}}}
\bibcite{vandeven2024continuallearningcatastrophicforgetting}{{109}{2024}{{van~de Ven et~al.}}{{van~de Ven, Soures, and Kudithipudi}}}
\bibcite{Vapnik1999-VAPTNO}{{110}{1999}{{Vapnik}}{{}}}
\bibcite{wang2019svmdsn}{{111}{2019}{{Wang et~al.}}{{Wang, Feng, and Wu}}}
\bibcite{wegel2025samplecomplexitysemisupervisedmultiobjective}{{112}{2025}{{Wegel et~al.}}{{Wegel, So, Park, and Yang}}}
\bibcite{Wegener1987}{{113}{1987}{{Wegener}}{{}}}
\bibcite{wei2022emergentabilitieslargelanguage}{{114}{2022}{{Wei et~al.}}{{Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang, Dean, and Fedus}}}
\bibcite{Yang_2024}{{115}{2024}{{Yang \& Suzuki}}{{Yang and Suzuki}}}
\bibcite{yang_rethinking_2020}{{116}{2020}{{Yang et~al.}}{{Yang, Yu, You, Steinhardt, and Ma}}}
\bibcite{zhang2023divedeeplearning}{{117}{2023}{{Zhang et~al.}}{{Zhang, Lipton, Li, and Smola}}}
\bibcite{zhang2017understandingdeeplearningrequires}{{118}{2017}{{Zhang et~al.}}{{Zhang, Bengio, Hardt, Recht, and Vinyals}}}
\bibcite{zhang_gradient_2019}{{119}{2019}{{Zhang}}{{}}}
\bibcite{zhang2014equivalence}{{120}{2014}{{Zhang \& Wang}}{{Zhang and Wang}}}
\bibcite{zhang2023mathematical}{{121}{2023}{{Zhang}}{{}}}
\bibcite{zhang2024manipulatingsparsedoubledescent}{{122}{2024}{{Zhang}}{{}}}
\bibstyle{iclr2025_conference}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Linear regression with Gaussian white noise, full control.}}{42}{algocf.1}\protected@file@percent }
\newlabel{algo:algo_reg_lin_1}{{1}{42}{}{algocf.1}{}}
\citation{shi2024homophilymodulatesdoubledescent}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Risk curves for varying sample size $n$ (dimension $d=100$, noise $\sigma =0.5$). Double descent is determined in similar sequence, even though the interpolation theoretical $p=n$ is actually shifted over to the right from the actual interpolation observables, in parameterization. Afterward, correlation fails, and the theoretical $p$ is ineffective, similar to previous theorem-based test run. Sample set size range is $n\in \{12,24,36,50,76,82,90,100,106,112,121,150,231,250,256,280\}$. One additional remark is that the error landscape is relatively thin in either side, making it abnormal in comparison to actual bias-variance curvature.}}{43}{figure.caption.17}\protected@file@percent }
\newlabel{fig:theorem_double_descent_fig4}{{12}{43}{Risk curves for varying sample size $n$ (dimension $d=100$, noise $\sigma =0.5$). Double descent is determined in similar sequence, even though the interpolation theoretical $p=n$ is actually shifted over to the right from the actual interpolation observables, in parameterization. Afterward, correlation fails, and the theoretical $p$ is ineffective, similar to previous theorem-based test run. Sample set size range is $n\in \{12,24,36,50,76,82,90,100,106,112,121,150,231,250,256,280\}$. One additional remark is that the error landscape is relatively thin in either side, making it abnormal in comparison to actual bias-variance curvature}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {.0.7}Remark}{43}{subsubsection.Alph0.0.7}\protected@file@percent }
\citation{10.5555/200548}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~.3\else \numberline {.3}Definition\fi \thmtformatoptarg {Representation scheme, structural definition}}{44}{definition.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \textbf  {Diagrammatic view for the ambiance space of the modelling scenario setting.} Under classical learning theory and consideration, such scheme is used for almost every aspect possible of the learner's action. $c\in \mathcal  {C}, h\in \mathcal  {H}$ described by the tuple $(\mathbf  {w},\mathbf  {s})$ of main (weight) parameters and special parameters (for example, bias), $\mathbf  {I}$ as the input space. The 3-tuple $(\omega , k,m)$ is used for controlling the partitioning (for $k=2$ is the train-test split). Others include $P(\mathbf  {I},c,h,\mathcal  {D}_{c})$ as the theorized action sequence (where the concept is exhibited, or the where the observational space is formed) of supposed distribution $\mathcal  {D}_{c}$, the supervisor $\text  {Supervisor}(\nabla , \mathcal  {A}, \{\Theta _{S}\})$ for the loss class $\nabla $, algorithm $\mathcal  {A}$, and $\{\Theta _{S}\}$ of special parameters for the supervisor. Additionally, we also include the supposed randomized state generation, $\mathsf  {RAND}_{i}(\theta _{i,j,k})$ of distinct controlling parameters and arbitrary pseudo-random shape.}}{45}{figure.caption.18}\protected@file@percent }
\newlabel{fig:vapnik_scheme}{{13}{45}{\textbf {Diagrammatic view for the ambiance space of the modelling scenario setting.} Under classical learning theory and consideration, such scheme is used for almost every aspect possible of the learner's action. $c\in \mathcal {C}, h\in \mathcal {H}$ described by the tuple $(\mathbf {w},\mathbf {s})$ of main (weight) parameters and special parameters (for example, bias), $\mathbf {I}$ as the input space. The 3-tuple $(\omega , k,m)$ is used for controlling the partitioning (for $k=2$ is the train-test split). Others include $P(\mathbf {I},c,h,\mathcal {D}_{c})$ as the theorized action sequence (where the concept is exhibited, or the where the observational space is formed) of supposed distribution $\mathcal {D}_{c}$, the supervisor $\text {Supervisor}(\nabla , \mathcal {A}, \{\Theta _{S}\})$ for the loss class $\nabla $, algorithm $\mathcal {A}$, and $\{\Theta _{S}\}$ of special parameters for the supervisor. Additionally, we also include the supposed randomized state generation, $\mathsf {RAND}_{i}(\theta _{i,j,k})$ of distinct controlling parameters and arbitrary pseudo-random shape}{figure.caption.18}{}}
\citation{brown2024biasvariance}
\citation{10.5555/2371238}
\citation{10.5555/2371238,lafon_understanding_2024}
\citation{adlam2020understandingdoubledescentrequires}
\citation{hastie2019surprises,mei2019generalization}
\citation{neal2018modern}
\citation{adlam2020understandingdoubledescentrequires}
\citation{hastie2019surprises,mei2019generalization}
\citation{neal2018modern}
\citation{adlam2020understandingdoubledescentrequires}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces (\textbf  {a-e) The different bias-variance decompositions.} (f-j) Corresponding theoretical predictions for $\gamma =0$, $\phi =1/16$ and $\sigma = \tanh $ with $\text  {SNR} = 100$ as the model capacity varies across the interpolation threshold (dashed red). (a,f) The semi-classical decomposition of~\cite  {hastie2019surprises,mei2019generalization} has a nonmonotonic and divergent bias term, conflicting with standard definitions of the bias. (b,g) The decomposition of~\cite  {neal2018modern} utilizing the law of total variance interprets the diverging term $V_D^\textsc  {c}$ as ``variance due to optimization''. (c,h) An alternative application of the law of total variance suggests the opposite, \emph  {i.e.} the diverging term $V_P^\textsc  {c}$ comes from ``variance due to sampling''. (d,i) A bivariate symmetric decomposition of the variance resolves this ambiguity and shows that the diverging term is actually $V_{PD}$, \emph  {i.e.} ``the variance explained by the parameters and data together beyond what they explain individually.'' (e,j) A trivariate symmetric decomposition reveals that the divergence comes from two terms, $V_{PX}$ and $V_{PX\bm  {\varepsilon }}$ (outlined in dashed red), and shows that label noise exacerbates but does not cause double descent. Since $V_{\varepsilon }=V_{P{\varepsilon }}=0$, they are not shown in (j). Taken from \cite  {adlam2020understandingdoubledescentrequires}}}{47}{figure.caption.19}\protected@file@percent }
\newlabel{fig:venn_variance}{{14}{47}{(\textbf {a-e) The different bias-variance decompositions.} (f-j) Corresponding theoretical predictions for $\gamma =0$, $\phi =1/16$ and $\fs = \tanh $ with $\text {SNR} = 100$ as the model capacity varies across the interpolation threshold (dashed red). (a,f) The semi-classical decomposition of~\cite {hastie2019surprises,mei2019generalization} has a nonmonotonic and divergent bias term, conflicting with standard definitions of the bias. (b,g) The decomposition of~\cite {neal2018modern} utilizing the law of total variance interprets the diverging term $V_D^\textsc {c}$ as ``variance due to optimization''. (c,h) An alternative application of the law of total variance suggests the opposite, \emph {i.e.} the diverging term $V_P^\textsc {c}$ comes from ``variance due to sampling''. (d,i) A bivariate symmetric decomposition of the variance resolves this ambiguity and shows that the diverging term is actually $V_{PD}$, \emph {i.e.} ``the variance explained by the parameters and data together beyond what they explain individually.'' (e,j) A trivariate symmetric decomposition reveals that the divergence comes from two terms, $V_{PX}$ and $V_{PX\bfe }$ (outlined in dashed red), and shows that label noise exacerbates but does not cause double descent. Since $V_\e =V_{P\e }=0$, they are not shown in (j). Taken from \cite {adlam2020understandingdoubledescentrequires}}{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces An illustration of the (supervised) statistical process. Phase III contains two parts: First is the evaluation $\nabla (h,c)$ according to the data $\mathcal  {D}$, and second is the $\mathsf  {Update}$ process to re-align $c$ to the actual target.}}{48}{figure.caption.20}\protected@file@percent }
\newlabel{fig:PhaseDiagram}{{15}{48}{An illustration of the (supervised) statistical process. Phase III contains two parts: First is the evaluation $\nabla (h,c)$ according to the data $\mathcal {D}$, and second is the $\mathsf {Update}$ process to re-align $c$ to the actual target}{figure.caption.20}{}}
\citation{10.5555/2930837,10.5555/200548}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~.4\else \numberline {.4}Definition\fi \thmtformatoptarg {Deterministic - \textit  {discriminative modelling}}}{49}{definition.4}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~.5\else \numberline {.5}Definition\fi \thmtformatoptarg {Probabilistic - \textit  {generative modelling}}}{49}{definition.5}\protected@file@percent }
\citation{6797087}
\citation{6797087}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~.6\else \numberline {.6}Definition\fi \thmtformatoptarg {Loss function}}{50}{definition.6}\protected@file@percent }
\@writefile{loe}{\contentsline {conjecture}{\ifthmt@listswap Conjecture~.1\else \numberline {.1}Conjecture\fi \thmtformatoptarg {Loss function convexity}}{50}{conjecture.1}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~.7\else \numberline {.7}Definition\fi \thmtformatoptarg {Parameterization}}{50}{definition.7}\protected@file@percent }
\gdef \@abspage@last{51}
