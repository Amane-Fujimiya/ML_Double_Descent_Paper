\relax 
\nicematrix@redefine@check@rerun 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{10.1145/1968.1972}
\citation{Vapnik1999-VAPTNO}
\citation{6797087}
\citation{belkin_reconciling_2019}
\citation{Vapnik1999-VAPTNO,10.5555/2371238,10.5555/2621980,STL_Hajek_Maxim_2021,bousquet2020theoryuniversallearning}
\citation{6797087,Domingos2000AUB}
\citation{belkin_reconciling_2019,schaeffer_double_2023,nakkiran_deep_2019,lafon_understanding_2024}
\citation{davies_unifying_2023,d_ascoli_triple_2020}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{belkin_reconciling_2019}
\citation{nakkiran_deep_2019}
\citation{Jacot:2018:NTK}
\citation{lafon_understanding_2024}
\citation{schaeffer_double_2023}
\citation{liu2023understandingroleoptimizationdouble}
\citation{davies_unifying_2023}
\citation{olmin2024understandingepochwisedoubledescent}
\citation{gareth_james_introduction_2013,goodfellow2016deep,STL_Hajek_Maxim_2021,10.5555/2371238,10.5555/2621980}
\citation{LehmannCasella_theory_1998,liam_statistics_2005}
\citation{6797087}
\citation{6797087}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem statements}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Bias-variance tradeoff}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Precursor (Geman et al., 1992)}{2}{subsubsection.2.1.1}\protected@file@percent }
\citation{brown2024biasvariance}
\citation{adlam2020understandingdoubledescentrequires}
\citation{brown2024biasvariance,PfauBregmanDivergence}
\citation{lafon_understanding_2024}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~2.1\else \numberline {2.1}Theorem\fi \thmtformatoptarg {Bias-variance decomposition}}{3}{theorem.1}\protected@file@percent }
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~2.2\else \numberline {2.2}Theorem\fi \thmtformatoptarg {Bias-variance tradeoff}}{3}{theorem.2}\protected@file@percent }
\citation{belkin_reconciling_2019}
\citation{belkin_reconciling_2019}
\citation{belkin_reconciling_2019}
\citation{belkin_reconciling_2019}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\citation{nakkiran_deep_2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Double descent}{4}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces {\bf  Curves for training risk (dashed line) and test risk (solid line).} ({\bf  a}) The classical \emph  {U-shaped risk curve} arising from the bias-variance trade-off. ({\bf  b}) The \emph  {double descent risk curve}, which incorporates the U-shaped risk curve (i.e., the ``classical'' regime) together with the observed behaviour from using high capacity function classes (i.e., the ``modern'' interpolating regime), separated by the interpolation threshold. The predictors to the right of the interpolation threshold have zero training risk. Reproduced from \cite  {belkin_reconciling_2019}.}}{4}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:double-descent}{{1}{4}{{\bf Curves for training risk (dashed line) and test risk (solid line).} ({\bf a}) The classical \emph {U-shaped risk curve} arising from the bias-variance trade-off. ({\bf b}) The \emph {double descent risk curve}, which incorporates the U-shaped risk curve (i.e., the ``classical'' regime) together with the observed behaviour from using high capacity function classes (i.e., the ``modern'' interpolating regime), separated by the interpolation threshold. The predictors to the right of the interpolation threshold have zero training risk. Reproduced from \cite {belkin_reconciling_2019}}{figure.caption.1}{}}
\citation{nakkiran_deep_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces {\bf  Left:} Train and test error as a function of model size, for ResNet18s of varying width on CIFAR-10 with 15\% label noise. {\bf  Right:} Test error, shown for varying train epochs. All models trained using Adam for 4K epochs. The largest model (width $64$) corresponds to standard ResNet18. Resued from \cite  {nakkiran_deep_2019}. }}{5}{figure.caption.2}\protected@file@percent }
\newlabel{fig:errorvscomplexity}{{2}{5}{{\bf Left:} Train and test error as a function of model size, for ResNet18s of varying width on CIFAR-10 with 15\% label noise. {\bf Right:} Test error, shown for varying train epochs. All models trained using Adam for 4K epochs. The largest model (width $64$) corresponds to standard ResNet18. Resued from \cite {nakkiran_deep_2019}}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces {\bf  Left:} Test error as a function of model size and train epochs. The horizontal line corresponds to model-wise double descent--varying model size while training for as long as possible. The vertical line corresponds to epoch-wise double descent, with test error undergoing double-descent as train time increases. {\bf  Right} Train error of the corresponding models. All models are Resnet18s trained on CIFAR-10 with 15\% label noise, data-augmentation, and Adam for up to 4K epochs. Reused from \cite  {nakkiran_deep_2019}}}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:unified}{{3}{5}{{\bf Left:} Test error as a function of model size and train epochs. The horizontal line corresponds to model-wise double descent--varying model size while training for as long as possible. The vertical line corresponds to epoch-wise double descent, with test error undergoing double-descent as train time increases. {\bf Right} Train error of the corresponding models. All models are Resnet18s trained on CIFAR-10 with 15\% label noise, data-augmentation, and Adam for up to 4K epochs. Reused from \cite {nakkiran_deep_2019}}{figure.caption.3}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~2.1\else \numberline {2.1}Definition\fi \thmtformatoptarg {Effective Model Complexity}}{5}{definition.1}\protected@file@percent }
\@writefile{loe}{\contentsline {hypothesis}{\ifthmt@listswap Hypothesis~2.1\else \numberline {2.1}Hypothesis\fi \thmtformatoptarg {Generalized Double Descent hypothesis, informal}}{5}{hypothesis.1}\protected@file@percent }
\newlabel{hyp:informaldd}{{2.1}{5}{Generalized Double Descent hypothesis, informal}{hypothesis.1}{}}
\citation{belkin_reconciling_2019}
\citation{advani2017highdimensionaldynamicsgeneralizationerror}
\citation{belkin2018understanddeeplearningneed}
\citation{mei2020generalizationerrorrandomfeatures}
\bibdata{references}
\bibcite{adlam2020understandingdoubledescentrequires}{{1}{2020}{{Adlam \& Pennington}}{{Adlam and Pennington}}}
\bibcite{advani2017highdimensionaldynamicsgeneralizationerror}{{2}{2017}{{Advani \& Saxe}}{{Advani and Saxe}}}
\bibcite{belkin2018understanddeeplearningneed}{{3}{2018}{{Belkin et~al.}}{{Belkin, Ma, and Mandal}}}
\bibcite{belkin_reconciling_2019}{{4}{2019}{{Belkin et~al.}}{{Belkin, Hsu, Ma, and Mandal}}}
\bibcite{bousquet2020theoryuniversallearning}{{5}{2020}{{Bousquet et~al.}}{{Bousquet, Hanneke, Moran, van Handel, and Yehudayoff}}}
\bibcite{brown2024biasvariance}{{6}{2024}{{Brown \& Ali}}{{Brown and Ali}}}
\bibcite{d_ascoli_triple_2020}{{7}{2020}{{d'~Ascoli et~al.}}{{d'~Ascoli, Sagun, and Biroli}}}
\bibcite{davies_unifying_2023}{{8}{2023}{{Davies et~al.}}{{Davies, Langosco, and Krueger}}}
\bibcite{Domingos2000AUB}{{9}{2000}{{Domingos}}{{}}}
\bibcite{LehmannCasella_theory_1998}{{10}{1998}{{E.~L.~Lehmann}}{{}}}
\bibcite{6797087}{{11}{1992}{{Geman et~al.}}{{Geman, Bienenstock, and Doursat}}}
\bibcite{goodfellow2016deep}{{12}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{STL_Hajek_Maxim_2021}{{13}{2021}{{Hajek \& Raginsky}}{{Hajek and Raginsky}}}
\bibcite{Jacot:2018:NTK}{{14}{2018}{{Jacot et~al.}}{{Jacot, Gabriel, and Hongler}}}
\bibcite{gareth_james_introduction_2013}{{15}{2013}{{James et~al.}}{{James, Hastie, Tibshirani, and Witten}}}
\bibcite{lafon_understanding_2024}{{16}{2024}{{Lafon \& Thomas}}{{Lafon and Thomas}}}
\bibcite{liu2023understandingroleoptimizationdouble}{{17}{2023}{{Liu \& Flanigan}}{{Liu and Flanigan}}}
\bibcite{mei2020generalizationerrorrandomfeatures}{{18}{2020}{{Mei \& Montanari}}{{Mei and Montanari}}}
\bibcite{10.5555/2371238}{{19}{2012}{{Mohri et~al.}}{{Mohri, Rostamizadeh, and Talwalkar}}}
\bibcite{nakkiran_deep_2019}{{20}{2019}{{Nakkiran et~al.}}{{Nakkiran, Kaplun, Bansal, Yang, Barak, and Sutskever}}}
\bibcite{olmin2024understandingepochwisedoubledescent}{{21}{2024}{{Olmin \& Lindsten}}{{Olmin and Lindsten}}}
\bibcite{liam_statistics_2005}{{22}{2005}{{Paninski}}{{}}}
\bibcite{PfauBregmanDivergence}{{23}{2013}{{Pfau}}{{}}}
\bibcite{schaeffer_double_2023}{{24}{2023}{{Schaeffer et~al.}}{{Schaeffer, Khona, Robertson, Boopathy, Pistunova, Rocks, Fiete, and Koyejo}}}
\bibcite{10.5555/2621980}{{25}{2014}{{Shalev-Shwartz \& Ben-David}}{{Shalev-Shwartz and Ben-David}}}
\bibcite{10.1145/1968.1972}{{26}{1984}{{Valiant}}{{}}}
\bibcite{Vapnik1999-VAPTNO}{{27}{1999}{{Vapnik}}{{}}}
\bibstyle{iclr2025_conference}
\gdef \@abspage@last{8}
