\relax 
\nicematrix@redefine@check@rerun 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{10.1145/1968.1972}
\citation{Vapnik1999-VAPTNO}
\citation{6797087}
\citation{belkin_reconciling_2019}
\citation{Vapnik1999-VAPTNO,10.5555/2371238,10.5555/2621980,STL_Hajek_Maxim_2021,bousquet2020theoryuniversallearning}
\citation{6797087,Domingos2000AUB}
\citation{belkin_reconciling_2019,schaeffer_double_2023,nakkiran_deep_2019,lafon_understanding_2024}
\citation{davies_unifying_2023,d_ascoli_triple_2020}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{belkin_reconciling_2019}
\citation{nakkiran_deep_2019}
\citation{Jacot:2018:NTK}
\citation{lafon_understanding_2024}
\citation{schaeffer_double_2023}
\citation{liu2023understandingroleoptimizationdouble}
\citation{davies_unifying_2023}
\citation{olmin2024understandingepochwisedoubledescent}
\citation{shi2024homophilymodulatesdoubledescent}
\citation{shi2024homophilymodulatesdoubledescent,buschjager_generalized_2020}
\citation{GRP_Hamilton}
\citation{shi2024homophilymodulatesdoubledescent}
\citation{shi2024homophilymodulatesdoubledescent}
\@writefile{toc}{\contentsline {section}{\numberline {2}Graph theoretical learning}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.1}Quick introduction to graph theory}{3}{subsubsection.3.0.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \textbf  {Illustration of an ordering for a specific graph configuration.} We emphasize the perspective to the higher neighbouring degree node in the graph, namely $V_{1}$, and their relative links and connections of the graph itself. A 2-walk (in red) can be seen, which aggregates more to create the 2-neighbourhood of maximally two walks away. }}{3}{figure.caption.1}\protected@file@percent }
\citation{GRP_Hamilton}
\citation{Oono2020Graph,lopushanskyy2024graphneuralnetworksgraph,Scar04,GRP_Hamilton}
\citation{rossi2020temporalgraphnetworksdeep}
\citation{GRP_Hamilton,Scar04}
\citation{Scar04,Veli_kovi__2023,tanis2024introductiongraphneuralnetworks,lopushanskyy2024graphneuralnetworksgraph}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustrative decomposition of the graph on its simplexes edges and vertices: Each of the decomposed space $\mathcal  {V}$ and $\mathcal  {E}$ can be encoded separately to their own ordeal, for example, of the edge connection through incident matrix.}}{4}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:graphdecom}{{2}{4}{Illustrative decomposition of the graph on its simplexes edges and vertices: Each of the decomposed space $\mathcal {V}$ and $\mathcal {E}$ can be encoded separately to their own ordeal, for example, of the edge connection through incident matrix}{figure.caption.2}{}}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~3.1\else \numberline {3.1}Definition\fi \thmtformatoptarg {Graph learning problem}}{4}{definition.1}\protected@file@percent }
\citation{pyg_docs,Fey/Lenssen/2019}
\citation{Scar04,GRP_Hamilton}
\citation{GRP_Hamilton}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.2}Graph Neural Network}{5}{subsubsection.3.0.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A conceptual illustration on the running flow of an $n$-layer GNN on particular structure of interest. Note that the data section itself has particular embedding structure on its own.}}{5}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Encoding space and the change of encoding inbetween a static graph (or snapshot-wise) GNN. The encoding space is warped toward arbitrary notions inside a GNN, to the point that after certain layers of processing, the encoding outputs different from expected encoding space (native to the model, not to the designer), though there might be universal notions preserved, like the degree of node represented in a different way.}}{6}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Double descent and GNN}{6}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Testing scenario}{6}{subsection.3.2}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~3.2\else \numberline {3.2}Definition\fi \thmtformatoptarg {Graph-theoretical problem}}{6}{definition.2}\protected@file@percent }
\@writefile{loe}{\contentsline {question}{\ifthmt@listswap Question~3.1\else \numberline {3.1}Question\fi }{7}{question.1}\protected@file@percent }
\bibdata{references}
\bibcite{belkin_reconciling_2019}{{1}{2019}{{Belkin et~al.}}{{Belkin, Hsu, Ma, and Mandal}}}
\bibcite{bousquet2020theoryuniversallearning}{{2}{2020}{{Bousquet et~al.}}{{Bousquet, Hanneke, Moran, van Handel, and Yehudayoff}}}
\bibcite{buschjager_generalized_2020}{{3}{2020}{{Buschjäger et~al.}}{{Buschjäger, Pfahler, and Morik}}}
\bibcite{d_ascoli_triple_2020}{{4}{2020}{{d'~Ascoli et~al.}}{{d'~Ascoli, Sagun, and Biroli}}}
\bibcite{davies_unifying_2023}{{5}{2023}{{Davies et~al.}}{{Davies, Langosco, and Krueger}}}
\bibcite{Domingos2000AUB}{{6}{2000}{{Domingos}}{{}}}
\bibcite{6797087}{{7}{1992}{{Geman et~al.}}{{Geman, Bienenstock, and Doursat}}}
\bibcite{STL_Hajek_Maxim_2021}{{8}{2021}{{Hajek \& Raginsky}}{{Hajek and Raginsky}}}
\bibcite{GRP_Hamilton}{{9}{}{{Hamilton}}{{}}}
\bibcite{Jacot:2018:NTK}{{10}{2018}{{Jacot et~al.}}{{Jacot, Gabriel, and Hongler}}}
\bibcite{lafon_understanding_2024}{{11}{2024}{{Lafon \& Thomas}}{{Lafon and Thomas}}}
\bibcite{liu2023understandingroleoptimizationdouble}{{12}{2023}{{Liu \& Flanigan}}{{Liu and Flanigan}}}
\bibcite{lopushanskyy2024graphneuralnetworksgraph}{{13}{2024}{{Lopushanskyy \& Shi}}{{Lopushanskyy and Shi}}}
\bibcite{10.5555/2371238}{{14}{2012}{{Mohri et~al.}}{{Mohri, Rostamizadeh, and Talwalkar}}}
\bibcite{nakkiran_deep_2019}{{15}{2019}{{Nakkiran et~al.}}{{Nakkiran, Kaplun, Bansal, Yang, Barak, and Sutskever}}}
\bibcite{olmin2024understandingepochwisedoubledescent}{{16}{2024}{{Olmin \& Lindsten}}{{Olmin and Lindsten}}}
\bibcite{Oono2020Graph}{{17}{2020}{{Oono \& Suzuki}}{{Oono and Suzuki}}}
\bibcite{Scar04}{{18}{2009}{{Scarselli et~al.}}{{Scarselli, Gori, Tsoi, Hagenbuchner, and Monfardini}}}
\bibcite{schaeffer_double_2023}{{19}{2023}{{Schaeffer et~al.}}{{Schaeffer, Khona, Robertson, Boopathy, Pistunova, Rocks, Fiete, and Koyejo}}}
\bibcite{10.5555/2621980}{{20}{2014}{{Shalev-Shwartz \& Ben-David}}{{Shalev-Shwartz and Ben-David}}}
\bibcite{shi2024homophilymodulatesdoubledescent}{{21}{2024}{{Shi et~al.}}{{Shi, Pan, Hu, and Dokmanić}}}
\bibcite{tanis2024introductiongraphneuralnetworks}{{22}{2024}{{Tanis et~al.}}{{Tanis, Giannella, and Mariano}}}
\bibcite{10.1145/1968.1972}{{23}{1984}{{Valiant}}{{}}}
\bibcite{Vapnik1999-VAPTNO}{{24}{1999}{{Vapnik}}{{}}}
\bibcite{Veli_kovi__2023}{{25}{2023}{{Veličković}}{{}}}
\bibstyle{iclr2025_conference}
\gdef \@abspage@last{9}
