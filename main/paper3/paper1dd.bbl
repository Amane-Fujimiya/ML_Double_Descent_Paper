\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and
  Mandal]{belkin_reconciling_2019}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine learning practice and the bias-variance
  trade-off.
\newblock \emph{Proc. Natl. Acad. Sci. U.S.A.}, 116\penalty0 (32):\penalty0
  15849--15854, August 2019.
\newblock ISSN 0027-8424, 1091-6490.
\newblock \doi{10.1073/pnas.1903070116}.
\newblock URL \url{http://arxiv.org/abs/1812.11118}.
\newblock arXiv:1812.11118 [cs, stat].

\bibitem[Bousquet et~al.(2020)Bousquet, Hanneke, Moran, van Handel, and
  Yehudayoff]{bousquet2020theoryuniversallearning}
Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon van Handel, and Amir
  Yehudayoff.
\newblock A theory of universal learning, 2020.
\newblock URL \url{https://arxiv.org/abs/2011.04483}.

\bibitem[Buschjäger et~al.(2020)Buschjäger, Pfahler, and
  Morik]{buschjager_generalized_2020}
Sebastian Buschjäger, Lukas Pfahler, and Katharina Morik.
\newblock Generalized {Negative} {Correlation} {Learning} for {Deep}
  {Ensembling}, December 2020.
\newblock URL \url{http://arxiv.org/abs/2011.02952}.
\newblock arXiv:2011.02952 [cs, stat].

\bibitem[d'~Ascoli et~al.(2020)d'~Ascoli, Sagun, and
  Biroli]{d_ascoli_triple_2020}
Stéphane d'~Ascoli, Levent Sagun, and Giulio Biroli.
\newblock Triple descent and the two kinds of overfitting: where \& why do they
  appear?
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~33, pp.\  3058--3069. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/1fd09c5f59a8ff35d499c0ee25a1d47e-Abstract.html}.

\bibitem[Davies et~al.(2023)Davies, Langosco, and
  Krueger]{davies_unifying_2023}
Xander Davies, Lauro Langosco, and David Krueger.
\newblock Unifying {Grokking} and {Double} {Descent}, March 2023.
\newblock URL \url{http://arxiv.org/abs/2303.06173}.
\newblock arXiv:2303.06173 [cs].

\bibitem[Domingos(2000)]{Domingos2000AUB}
Pedro~M. Domingos.
\newblock A unified bias-variance decomposition for zero-one and squared loss.
\newblock In \emph{AAAI/IAAI}, 2000.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:2063488}.

\bibitem[Geman et~al.(1992)Geman, Bienenstock, and Doursat]{6797087}
Stuart Geman, Elie Bienenstock, and René Doursat.
\newblock Neural networks and the bias/variance dilemma.
\newblock \emph{Neural Computation}, 4\penalty0 (1):\penalty0 1--58, 1992.
\newblock \doi{10.1162/neco.1992.4.1.1}.

\bibitem[Hajek \& Raginsky(2021)Hajek and Raginsky]{STL_Hajek_Maxim_2021}
Bruce Hajek and Maxim Raginsky.
\newblock \emph{Statistical Learning Theory}, volume~1.
\newblock 2021.
\newblock URL \url{https://maxim.ece.illinois.edu/teaching/SLT/}.

\bibitem[Hamilton()]{GRP_Hamilton}
William~L. Hamilton.
\newblock Graph representation learning.
\newblock \emph{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}, 14\penalty0 (3):\penalty0 1--159.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{Jacot:2018:NTK}
Arthur Jacot, Fran\c~cois Gabriel, and Clément Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.
\newblock Kernel view of wide-network behavior.

\bibitem[Lafon \& Thomas(2024)Lafon and Thomas]{lafon_understanding_2024}
Marc Lafon and Alexandre Thomas.
\newblock Understanding the {Double} {Descent} {Phenomenon} in {Deep}
  {Learning}, March 2024.
\newblock URL \url{http://arxiv.org/abs/2403.10459}.
\newblock arXiv:2403.10459 [cs, stat].

\bibitem[Liu \& Flanigan(2023)Liu and
  Flanigan]{liu2023understandingroleoptimizationdouble}
Chris~Yuhao Liu and Jeffrey Flanigan.
\newblock Understanding the role of optimization in double descent, 2023.
\newblock URL \url{https://arxiv.org/abs/2312.03951}.

\bibitem[Lopushanskyy \& Shi(2024)Lopushanskyy and
  Shi]{lopushanskyy2024graphneuralnetworksgraph}
Dmytro Lopushanskyy and Borun Shi.
\newblock Graph neural networks on graph databases, 2024.
\newblock URL \url{https://arxiv.org/abs/2411.11375}.

\bibitem[Mohri et~al.(2012)Mohri, Rostamizadeh, and Talwalkar]{10.5555/2371238}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock \emph{Foundations of Machine Learning}.
\newblock The MIT Press, 2012.
\newblock ISBN 026201825X.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran_deep_2019}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data}
  {Hurt}, December 2019.
\newblock URL \url{http://arxiv.org/abs/1912.02292}.
\newblock arXiv:1912.02292 [cs, stat].

\bibitem[Olmin \& Lindsten(2024)Olmin and
  Lindsten]{olmin2024understandingepochwisedoubledescent}
Amanda Olmin and Fredrik Lindsten.
\newblock Towards understanding epoch-wise double descent in two-layer linear
  neural networks, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.09845}.

\bibitem[Oono \& Suzuki(2020)Oono and Suzuki]{Oono2020Graph}
Kenta Oono and Taiji Suzuki.
\newblock Graph neural networks exponentially lose expressive power for node
  classification.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=S1ldO2EFPr}.

\bibitem[Scarselli et~al.(2009)Scarselli, Gori, Tsoi, Hagenbuchner, and
  Monfardini]{Scar04}
Franco Scarselli, Marco Gori, Ah~Chung Tsoi, Markus Hagenbuchner, and Gabriele
  Monfardini.
\newblock The graph neural network model.
\newblock \emph{IEEE Transactions on Neural Networks}, 20\penalty0
  (1):\penalty0 61--80, 2009.
\newblock \doi{10.1109/TNN.2008.2005605}.

\bibitem[Schaeffer et~al.(2023)Schaeffer, Khona, Robertson, Boopathy,
  Pistunova, Rocks, Fiete, and Koyejo]{schaeffer_double_2023}
Rylan Schaeffer, Mikail Khona, Zachary Robertson, Akhilan Boopathy, Kateryna
  Pistunova, Jason~W. Rocks, Ila~Rani Fiete, and Oluwasanmi Koyejo.
\newblock Double {Descent} {Demystified}: {Identifying}, {Interpreting} \&
  {Ablating} the {Sources} of a {Deep} {Learning} {Puzzle}, March 2023.
\newblock URL \url{http://arxiv.org/abs/2303.14151}.
\newblock arXiv:2303.14151 [cs, stat].

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and
  Ben-David]{10.5555/2621980}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock \emph{Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, USA, 2014.
\newblock ISBN 1107057132.

\bibitem[Shi et~al.(2024)Shi, Pan, Hu, and
  Dokmanić]{shi2024homophilymodulatesdoubledescent}
Cheng Shi, Liming Pan, Hong Hu, and Ivan Dokmanić.
\newblock Homophily modulates double descent generalization in graph
  convolution networks, 2024.
\newblock URL \url{https://arxiv.org/abs/2212.13069}.

\bibitem[Tanis et~al.(2024)Tanis, Giannella, and
  Mariano]{tanis2024introductiongraphneuralnetworks}
James~H. Tanis, Chris Giannella, and Adrian~V. Mariano.
\newblock Introduction to graph neural networks: A starting point for machine
  learning engineers, 2024.
\newblock URL \url{https://arxiv.org/abs/2412.19419}.

\bibitem[Valiant(1984)]{10.1145/1968.1972}
L.~G. Valiant.
\newblock A theory of the learnable.
\newblock \emph{Commun. ACM}, 27\penalty0 (11):\penalty0 1134–1142, November
  1984.
\newblock ISSN 0001-0782.
\newblock \doi{10.1145/1968.1972}.
\newblock URL \url{https://doi.org/10.1145/1968.1972}.

\bibitem[Vapnik(1999)]{Vapnik1999-VAPTNO}
Vladimir Vapnik.
\newblock \emph{The Nature of Statistical Learning Theory}.
\newblock Springer: New York, 1999.

\bibitem[Veličković(2023)]{Veli_kovi__2023}
Petar Veličković.
\newblock Everything is connected: Graph neural networks.
\newblock \emph{Current Opinion in Structural Biology}, 79:\penalty0 102538,
  April 2023.
\newblock ISSN 0959-440X.
\newblock \doi{10.1016/j.sbi.2023.102538}.
\newblock URL \url{http://dx.doi.org/10.1016/j.sbi.2023.102538}.

\end{thebibliography}
