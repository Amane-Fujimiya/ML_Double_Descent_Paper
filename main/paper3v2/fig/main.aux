\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{pnas-new}
\citation{lam2022graphcast}
\citation{mandal2022robust}
\citation{estrach2014spectral,defferrard2016convolutional,kipf2017semisupervised,hamilton2017inductive}
\citation{Oono2020Graph}
\babel@aux{english}{}
\zref@newlabel{mdf@pagelabel-1}{\default{\caption@xref {??}{ on input line 104}}\page{1}\abspage{1}\mdf@pagevalue{1}}
\pgfsyspdfmark {pgfid1}{26609889}{23403045}
\zref@newlabel{mdf@pagelabel-2}{\default{\caption@xref {??}{ on input line 104}}\page{1}\abspage{1}\mdf@pagevalue{1}}
\pgfsyspdfmark {pgfid2}{26609889}{5997650}
\citation{nakkiran2021deep}
\citation{belkin2020two}
\citation{mcpherson2001birds}
\citation{pei2020geom,chien2021adaptive}
\citation{wei2022understanding,baranwal2023optimality}
\citation{garg2020generalization,liao2021a,esser2021learning}
\citation{deshpande2018contextual}
\citation{oymak2013squared,thrampoulidis2018precise,boyd2011distributed}
\citation{belkin2020two,hu2022universality,mei2022generalization}
\citation{el2018detection,macris2020all,mignacco2020role}
\citation{belkin2019reconciling}
\citation{nakkiran2021deep}
\citation{Oono2020Graph}
\newlabel{eqn: risk_N}{{1}{2}{}{equation.0.1}{}}
\newlabel{eqn: loss_N}{{2}{2}{}{equation.0.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Double descent}{2}{figure.caption.1}\protected@file@percent }
\citation{belkin2020two}
\citation{sen2008collective}
\citation{rozemberczki2021multi}
\citation{pei2020geom}
\citation{maurya2021improving}
\citation{maurya2021improving}
\citation{velivckovic2017graph}
\citation{hamilton2017inductive}
\citation{defferrard2016convolutional}
\citation{chien2021adaptive}
\citation{chien2021adaptive,pei2020geom}
\citation{chien2021adaptive}
\citation{chien2021adaptive,pei2020geom}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Double descent for different GNNs and datasets. We show test loss (\textcolor {red}{\textbf  {red}}) and test accuracy (\textbf  {black}) under different training ratios $\tau $ on the abscissa, with different GCN settings, for 5 datasets, each in one row. \textbf  {First column}: one linear layer (O) trained by MSE loss (M); \textbf  {second column}: a two-layer GCN (T) with ReLU activations (R) and the MSE loss (M); \textbf  {third column}: a two-layer GCN (T) with ReLU activation function (R), dropout (D), and MSE loss (M); \textbf  {forth column}: a two-layer GCN (T) with ReLU activations (R) and cross-entropy loss (C); \textbf  {fifth column}: T+C+R+D. Each experiemntal data point is averaged over 10 random test--train splits.\relax }}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: Real DD}{{1}{3}{Double descent for different GNNs and datasets. We show test loss (\textcolor {red}{\textbf {red}}) and test accuracy (\textbf {black}) under different training ratios $\tau $ on the abscissa, with different GCN settings, for 5 datasets, each in one row. \textbf {First column}: one linear layer (O) trained by MSE loss (M); \textbf {second column}: a two-layer GCN (T) with ReLU activations (R) and the MSE loss (M); \textbf {third column}: a two-layer GCN (T) with ReLU activation function (R), dropout (D), and MSE loss (M); \textbf {forth column}: a two-layer GCN (T) with ReLU activations (R) and cross-entropy loss (C); \textbf {fifth column}: T+C+R+D. Each experiemntal data point is averaged over 10 random test--train splits.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Exp}{3}{figure.caption.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Test loss with different training ratios for different GCN structures on \texttt  {chameleon} (heterophilic) datasets. (A): \texttt  {FSGNN}\cite  {maurya2021improving}; (B): two layers GCN with ReLU activation function and cross entropy loss; (C): one layer GCN with cross entropy loss; (D): one layer GCN with MSE loss. The graphs are perturbed by randomly removing $t\%$ (illustrated by different colors on the top) existing edges and introducing an equivalent number of new random links. A $t$ value of $0$ represents the original graph, while $t=100$ corresponds to an Erdős–Rényi (ER) graph. Each data point is averaged ten times, and the x-axis is plotted on a logarithmic scale. The corresponding accuracy is shown in the SI Appendix?? {\relax \fontsize  {7}{8}\selectfont  \abovedisplayskip 4\p@ plus2\p@ minus2\p@ \abovedisplayshortskip \z@ plus1\p@ \belowdisplayshortskip 2.5\p@ plus\p@ minus\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 6\p@ plus2\p@ minus3\p@ \itemsep \parsep {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \color  {red}{\textsf  { Do we this this accuracy in the appendix??}}}\relax }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:chameleon4_loss}{{2}{3}{Test loss with different training ratios for different GCN structures on \texttt {chameleon} (heterophilic) datasets. (A): \texttt {FSGNN}\cite {maurya2021improving}; (B): two layers GCN with ReLU activation function and cross entropy loss; (C): one layer GCN with cross entropy loss; (D): one layer GCN with MSE loss. The graphs are perturbed by randomly removing $t\%$ (illustrated by different colors on the top) existing edges and introducing an equivalent number of new random links. A $t$ value of $0$ represents the original graph, while $t=100$ corresponds to an Erdős–Rényi (ER) graph. Each data point is averaged ten times, and the x-axis is plotted on a logarithmic scale. The corresponding accuracy is shown in the SI Appendix?? \check {Do we this this accuracy in the appendix??}\relax }{figure.caption.2}{}}
\citation{wu2019simplifying}
\citation{wang2022powerful,he2021bernnet}
\citation{wu2019simplifying}
\citation{maurya2021improving}
\citation{chien2021adaptive}
\citation{deshpande2018contextual}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Do we this this accuracy in the appendix?\relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:chameleon4_acc}{{3}{4}{Do we this this accuracy in the appendix?\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces GPRGNN\cite  {chien2021adaptive} on different datasets. The y-axis shows relative difference of prediction accuracy between dense splits (60\% training labels) and sparse splits (2.5\% training labels for homophilic datasets and 30\% for heterophilic datasets). The x-axis quantifies absolute homophily score of these datasets computed in \cite  {chien2021adaptive,pei2020geom}. We use $2H(\mathcal  {G})-1$ for homophilic datasets (${\textcolor {myblue} \bullet }$) and ${1-2H(\mathcal  {G})}$ for heterophilic datasets (${\textcolor {myred} \bullet })$. The dashed line represents the synthetic datasets from CSBM with ${\textcolor {mygreen} {--}}$ for GPRGNN and ${\textcolor {myyellow} {--}}$ for one layer GCN (with selfloop) which will be explicitly analysed in our paper.\relax }}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:GPRGNN}{{4}{4}{GPRGNN\cite {chien2021adaptive} on different datasets. The y-axis shows relative difference of prediction accuracy between dense splits (60\% training labels) and sparse splits (2.5\% training labels for homophilic datasets and 30\% for heterophilic datasets). The x-axis quantifies absolute homophily score of these datasets computed in \cite {chien2021adaptive,pei2020geom}. We use $2H(\mathcal {G})-1$ for homophilic datasets (${\textcolor {myblue} \bullet }$) and ${1-2H(\mathcal {G})}$ for heterophilic datasets (${\textcolor {myred} \bullet })$. The dashed line represents the synthetic datasets from CSBM with ${\textcolor {mygreen} {--}}$ for GPRGNN and ${\textcolor {myyellow} {--}}$ for one layer GCN (with selfloop) which will be explicitly analysed in our paper.\relax }{figure.caption.4}{}}
\newlabel{eq:lin-gnc}{{3}{4}{}{equation.0.3}{}}
\newlabel{eqn: optim}{{4}{4}{}{equation.0.4}{}}
\newlabel{eqn: risk}{{5}{4}{}{equation.0.5}{}}
\newlabel{eqn: ACC}{{6}{4}{}{equation.0.6}{}}
\citation{wang1987stochastic,malliaros2013clustering}
\citation{lu2021learning,baranwal2021graph}
\newlabel{eqn: A bs}{{7}{5}{}{equation.0.7}{}}
\newlabel{eqn: A bn}{{8}{5}{}{equation.0.8}{}}
\newlabel{eqn: scm}{{9}{5}{}{equation.0.9}{}}
\newlabel{sec:phenomena}{{}{5}{}{equation.0.10}{}}
\newlabel{SEC: dd in risks}{{}{5}{}{equation.0.10}{}}
\citation{li2022finding,luan2021heterophily,chien2021adaptive}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces {\relax \fontsize  {7}{8}\selectfont  \abovedisplayskip 4\p@ plus2\p@ minus2\p@ \abovedisplayshortskip \z@ plus1\p@ \belowdisplayshortskip 2.5\p@ plus\p@ minus\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 6\p@ plus2\p@ minus3\p@ \itemsep \parsep {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \color  {orange}{\textsf  {new: add y label.}}} Theoretical results computed by the replica method (solid line) vs. experimental results (round dots) on the CSBM, with $\ensuremath  {\bm  {{P}}}(\ensuremath  {\bm  {{A}}})=\ensuremath  {\bm  {{A}}}$, for varying training ratios $\tau $. (A): training and test risks with $\lambda =\mu =1$, $\gamma =5$ and $r=0$. (For $\tau <0.2$, we use the pseudoinverse in Equation \ref {eqn: w star} in numerics and $r=10^{-5}$ for the theoretical curves). (B) and (C): test risk with $r=0.02$, $\gamma =2$, $\mu =1$ in (B) and $\lambda =3$, $\mu =1$, $\gamma =2$ in (C). In all plots we set $N=5000$ and $d=30$. {\relax \fontsize  {7}{8}\selectfont  \abovedisplayskip 4\p@ plus2\p@ minus2\p@ \abovedisplayshortskip \z@ plus1\p@ \belowdisplayshortskip 2.5\p@ plus\p@ minus\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 6\p@ plus2\p@ minus3\p@ \itemsep \parsep {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \color  {orange}{\textsf  {new: We use the symmetric binary adjacency matrix set $\mathcal  {A}^{\text  {bs}}$.}}} Each experimental data point is averaged over $10$ independent trials and the standard deviation is indicated by vertical lines.\relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig_v}{{5}{6}{\add {add y label.} Theoretical results computed by the replica method (solid line) vs. experimental results (round dots) on the CSBM, with $\mP (\mA )=\mA $, for varying training ratios $\tau $. (A): training and test risks with $\lambda =\mu =1$, $\gamma =5$ and $r=0$. (For $\tau <0.2$, we use the pseudoinverse in Equation \ref {eqn: w star} in numerics and $r=10^{-5}$ for the theoretical curves). (B) and (C): test risk with $r=0.02$, $\gamma =2$, $\mu =1$ in (B) and $\lambda =3$, $\mu =1$, $\gamma =2$ in (C). In all plots we set $N=5000$ and $d=30$. \add {We use the symmetric binary adjacency matrix set $\mathcal {A}^{\text {bs}}$.} Each experimental data point is averaged over $10$ independent trials and the standard deviation is indicated by vertical lines.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces {\relax \fontsize  {7}{8}\selectfont  \abovedisplayskip 4\p@ plus2\p@ minus2\p@ \abovedisplayshortskip \z@ plus1\p@ \belowdisplayshortskip 2.5\p@ plus\p@ minus\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 6\p@ plus2\p@ minus3\p@ \itemsep \parsep {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \color  {orange}{\textsf  {new: new plot}}} Different levels of homophily lead to various types of double descent in the CSBM model. We illustrate the test risk when varying the sample ratio \(\alpha = 1/\gamma \), with \(\tau = 0.75, \mu = 1, r = 0.05\), under different homophily (\(\lambda \)). The graph structures range from random on the left to cleaner on the right in above four plots.\relax }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig: double descent with different alpha}{{6}{6}{\add {new plot} Different levels of homophily lead to various types of double descent in the CSBM model. We illustrate the test risk when varying the sample ratio \(\alpha = 1/\gamma \), with \(\tau = 0.75, \mu = 1, r = 0.05\), under different homophily (\(\lambda \)). The graph structures range from random on the left to cleaner on the right in above four plots.\relax }{figure.caption.6}{}}
\newlabel{SEC: dd in real}{{}{6}{}{figure.caption.7}{}}
\newlabel{SEC: sefl loops}{{}{6}{}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Four double descent curves. We show test accuracy and test risk for varying training ratios $\tau $. (A): increasing ACC with large $r$; (B): double descent ACC with small $r$; (C): double descent ACC with $\gamma $ close to $1$; and (D): (almost) decreasing ACC with small $\gamma $. We compare our theoretical results (solid line) with computer experiments (round dots) with $\ensuremath  {\bm  {{P}}}(\ensuremath  {\bm  {{A}}})=\ensuremath  {\bm  {{A}}}$ under different training ratios $\tau $. The parameters per columns are chosen as: (A) $\mu =1,\lambda =2,\gamma =5,r=2$; (B): $\mu =1,\lambda =2,\gamma =5,r=0.1$; (C): $\mu =1,\lambda =2,\gamma =1.2,r=0.05$; (D): $\lambda =1,\mu =5,\gamma =0.1,r=0.005$. We set $N=5000$ and $d=30$ for the first three plots, and $N=500$ and $d=20$ for the last plot. {\relax \fontsize  {7}{8}\selectfont  \abovedisplayskip 4\p@ plus2\p@ minus2\p@ \abovedisplayshortskip \z@ plus1\p@ \belowdisplayshortskip 2.5\p@ plus\p@ minus\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 6\p@ plus2\p@ minus3\p@ \itemsep \parsep {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \color  {orange}{\textsf  {new: We use the symmetric binary adjacency matrix set $\mathcal  {A}^{\text  {bs}}$.}}} Each experimental data point is averaged over $10$ independent trials and the standard deviation is indicated by vertical lines.\relax }}{6}{figure.caption.7}\protected@file@percent }
\newlabel{fig_acc}{{7}{6}{Four double descent curves. We show test accuracy and test risk for varying training ratios $\tau $. (A): increasing ACC with large $r$; (B): double descent ACC with small $r$; (C): double descent ACC with $\gamma $ close to $1$; and (D): (almost) decreasing ACC with small $\gamma $. We compare our theoretical results (solid line) with computer experiments (round dots) with $\mP (\mA )=\mA $ under different training ratios $\tau $. The parameters per columns are chosen as: (A) $\mu =1,\lambda =2,\gamma =5,r=2$; (B): $\mu =1,\lambda =2,\gamma =5,r=0.1$; (C): $\mu =1,\lambda =2,\gamma =1.2,r=0.05$; (D): $\lambda =1,\mu =5,\gamma =0.1,r=0.005$. We set $N=5000$ and $d=30$ for the first three plots, and $N=500$ and $d=20$ for the last plot. \add {We use the symmetric binary adjacency matrix set $\mathcal {A}^{\text {bs}}$.} Each experimental data point is averaged over $10$ independent trials and the standard deviation is indicated by vertical lines.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Double descents of the test loss (\textcolor {red}{\textbf  {red}}) and test accuracy (\textcolor {black}{\textbf  {black}}) on \texttt  {Cora} under different training ratios $\tau $ (abscissa) with a two-layer ReLU GCN. (A): MSE loss; (B): cross-entropy loss. {\relax \fontsize  {7}{8}\selectfont  \abovedisplayskip 4\p@ plus2\p@ minus2\p@ \abovedisplayshortskip \z@ plus1\p@ \belowdisplayshortskip 2.5\p@ plus\p@ minus\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 6\p@ plus2\p@ minus3\p@ \itemsep \parsep {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \color  {orange}{\textsf  {new: not not referenced anywhere, maybe delete?}}}\relax }}{6}{figure.caption.8}\protected@file@percent }
\newlabel{fig: Real DD typical}{{8}{6}{Double descents of the test loss (\textcolor {red}{\textbf {red}}) and test accuracy (\textcolor {black}{\textbf {black}}) on \texttt {Cora} under different training ratios $\tau $ (abscissa) with a two-layer ReLU GCN. (A): MSE loss; (B): cross-entropy loss. \add {not not referenced anywhere, maybe delete?}\relax }{figure.caption.8}{}}
\citation{maurya2021improving}
\citation{pei2020geom}
\citation{pei2020geom}
\citation{deshpande2018contextual}
\citation{deshpande2018contextual}
\citation{zhang2014phase}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces {\relax \fontsize  {7}{8}\selectfont  \abovedisplayskip 4\p@ plus2\p@ minus2\p@ \abovedisplayshortskip \z@ plus1\p@ \belowdisplayshortskip 2.5\p@ plus\p@ minus\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 6\p@ plus2\p@ minus3\p@ \itemsep \parsep {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \color  {orange}{\textsf  {new: add y label.}}} Risks on the CSBM with different self-loop intensity. (A): training and test risk for $\tau =0.8$ and $\lambda =-1$ (heterophilic). (B): test risks for $\gamma =0.8$, $\tau =0.8$ and $\mu =0$ for different $\lambda $. (C): training loss for different $\mu $ when $\tau =\lambda =1$. Each experimental data point is averaged over $10$ independent trials with $N=5000$, $r=0$, and $d=30$. We use the non-symmetric binary adjacency matrix set $\mathcal  {A}^{\text  {bn}}$. The solid lines are the theoretical results from the replica method.\relax }}{7}{figure.caption.9}\protected@file@percent }
\newlabel{fig_selfloop}{{9}{7}{\add {add y label.} Risks on the CSBM with different self-loop intensity. (A): training and test risk for $\tau =0.8$ and $\lambda =-1$ (heterophilic). (B): test risks for $\gamma =0.8$, $\tau =0.8$ and $\mu =0$ for different $\lambda $. (C): training loss for different $\mu $ when $\tau =\lambda =1$. Each experimental data point is averaged over $10$ independent trials with $N=5000$, $r=0$, and $d=30$. We use the non-symmetric binary adjacency matrix set $\mathcal {A}^{\text {bn}}$. The solid lines are the theoretical results from the replica method.\relax }{figure.caption.9}{}}
\newlabel{SEC: self-loop in real}{{}{7}{}{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of test accuracy without and with the negative self-loop. The datasets and splits are the same as for Figure \ref {fig_selfloop_real}.\relax }}{7}{table.caption.11}\protected@file@percent }
\newlabel{tab:fsgnn}{{1}{7}{Comparison of test accuracy without and with the negative self-loop. The datasets and splits are the same as for Figure \ref {fig_selfloop_real}.\relax }{table.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Node classification accuracy (\textcolor {black}{\textbf  {black}}) and test loss (\textcolor {red}{\textbf  {red}}) on real heterophilic graphs with different self-loop intensity on GCN. We implement a two-layer ReLU GCN with $128$ hidden neurons and an additional self-loop with strength $c$. Every result is averaged over different training-test splits taken from \cite  {pei2020geom}(60\% training, 20\% validation, 20\% test). Standard deviation (vertical line) mainly comes from the randomness of the splits, yet the randomness of the model and optimizer is relatively small.\relax }}{7}{figure.caption.10}\protected@file@percent }
\newlabel{fig_selfloop_real}{{10}{7}{Node classification accuracy (\textcolor {black}{\textbf {black}}) and test loss (\textcolor {red}{\textbf {red}}) on real heterophilic graphs with different self-loop intensity on GCN. We implement a two-layer ReLU GCN with $128$ hidden neurons and an additional self-loop with strength $c$. Every result is averaged over different training-test splits taken from \cite {pei2020geom}(60\% training, 20\% validation, 20\% test). Standard deviation (vertical line) mainly comes from the randomness of the splits, yet the randomness of the model and optimizer is relatively small.\relax }{figure.caption.10}{}}
\newlabel{SEC: main techniques}{{}{7}{}{figure.caption.12}{}}
\citation{mezard1987spin}
\citation{talagrand2002gaussian,carmona2006universality,panchenko2013sherrington}
\citation{deshpande2018contextual}
\citation{deshpande2018contextual}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Training ratio at which a one-layer GCN matches performance of unsupervised belief propagation. The dashed line is the information-theoretic detection threshold when the data is ($\ensuremath  {\bm  {{A}}}$, $\ensuremath  {\bm  {{X}}}$) but no labels are revealed.\relax }}{8}{figure.caption.12}\protected@file@percent }
\newlabel{fig:super_vs_unsuper_heatmap}{{11}{8}{Training ratio at which a one-layer GCN matches performance of unsupervised belief propagation. The dashed line is the information-theoretic detection threshold when the data is ($\mA $, $\mX $) but no labels are revealed.\relax }{figure.caption.12}{}}
\newlabel{eqn: w star}{{11}{8}{}{equation.0.11}{}}
\newlabel{eqn: z short}{{12}{8}{}{equation.0.12}{}}
\newlabel{eqn: sketch saddle point}{{13}{8}{}{equation.0.13}{}}
\newlabel{eqn: A gn}{{14}{8}{}{equation.0.14}{}}
\newlabel{eqn: A gs}{{15}{8}{}{equation.0.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Experimental validation of Conjecture \ref {conj: Equivalence}. In (A) \& (D): we show training and test risks with different numbers of nodes for $P(A)=A$, with $\gamma =\lambda =\mu =2,r=0.01,\tau =0.8$ and $d=\sqrt  {N}/2$. In (B) \& (E), we show the absolute values of difference between the risks with the binary and with the corresponding Gaussian adjacency, as a function of $N$, for data in (A) \& (D). We perform a linear regression on the data points with the logarithmic scale, and find that the error scales as $\vert \Delta \vert \sim N^{-0.5}$. In (C) \& (F) we show the training and test risks for different average node degrees $d$ and $\ensuremath  {\bm  {{P}}}(\ensuremath  {\bm  {{A}}})=\ensuremath  {\bm  {{A}}}^2$. $\lambda =\mu =1,\gamma =2$ and $N=2000$. We set $\gamma =0.5,\tau =0.8,r=0.01$.\relax }}{9}{figure.caption.13}\protected@file@percent }
\newlabel{fig: conjecture}{{12}{9}{Experimental validation of Conjecture \ref {conj: Equivalence}. In (A) \& (D): we show training and test risks with different numbers of nodes for $P(A)=A$, with $\gamma =\lambda =\mu =2,r=0.01,\tau =0.8$ and $d=\sqrt {N}/2$. In (B) \& (E), we show the absolute values of difference between the risks with the binary and with the corresponding Gaussian adjacency, as a function of $N$, for data in (A) \& (D). We perform a linear regression on the data points with the logarithmic scale, and find that the error scales as $\vert \Delta \vert \sim N^{-0.5}$. In (C) \& (F) we show the training and test risks for different average node degrees $d$ and $\mP (\mA )=\mA ^2$. $\lambda =\mu =1,\gamma =2$ and $N=2000$. We set $\gamma =0.5,\tau =0.8,r=0.01$.\relax }{figure.caption.13}{}}
\newlabel{conj: Equivalence}{{1}{9}{Equivalence of graph matrices}{conjecture.1}{}}
\citation{sato2020survey,geerts2022expressiveness}
\citation{xu2018how}
\citation{gilmer2017neural}
\citation{scarselli2018vapnik}
\citation{garg2020generalization}
\citation{liao2021a}
\citation{esser2021learning}
\citation{garg2020generalization,liao2021a,esser2021learning}
\citation{oymak2013squared,thrampoulidis2018precise,boyd2011distributed}
\citation{belkin2020two,hu2022universality,mei2022generalization}
\citation{el2018detection,macris2020all,mignacco2020role}
\citation{deshpande2017asymptotic,mossel2018proof,el2018detection}
\citation{deshpande2018contextual,deshpande2017asymptotic}
\citation{liao2021a}
\newlabel{eqn: Risk r>=0}{{16}{10}{}{equation.0.16}{}}
\newlabel{eqn: Risk r=0}{{17}{10}{}{equation.0.17}{}}
\citation{pennington2017nonlinear}
\citation{keriven2022not}
\bibdata{pnas-sample}
\bibcite{lam2022graphcast}{{1}{}{{}}{{}}}
\bibcite{mandal2022robust}{{2}{}{{}}{{}}}
\bibcite{estrach2014spectral}{{3}{}{{}}{{}}}
\bibcite{defferrard2016convolutional}{{4}{}{{}}{{}}}
\bibcite{kipf2017semisupervised}{{5}{}{{}}{{}}}
\bibcite{hamilton2017inductive}{{6}{}{{}}{{}}}
\bibcite{Oono2020Graph}{{7}{}{{}}{{}}}
\bibcite{nakkiran2021deep}{{8}{}{{}}{{}}}
\bibcite{belkin2020two}{{9}{}{{}}{{}}}
\bibcite{mcpherson2001birds}{{10}{}{{}}{{}}}
\bibcite{pei2020geom}{{11}{}{{}}{{}}}
\bibcite{chien2021adaptive}{{12}{}{{}}{{}}}
\bibcite{wei2022understanding}{{13}{}{{}}{{}}}
\bibcite{baranwal2023optimality}{{14}{}{{}}{{}}}
\bibcite{garg2020generalization}{{15}{}{{}}{{}}}
\bibcite{liao2021a}{{16}{}{{}}{{}}}
\bibcite{esser2021learning}{{17}{}{{}}{{}}}
\bibcite{deshpande2018contextual}{{18}{}{{}}{{}}}
\bibcite{oymak2013squared}{{19}{}{{}}{{}}}
\bibcite{thrampoulidis2018precise}{{20}{}{{}}{{}}}
\bibcite{boyd2011distributed}{{21}{}{{}}{{}}}
\bibcite{hu2022universality}{{22}{}{{}}{{}}}
\bibcite{mei2022generalization}{{23}{}{{}}{{}}}
\bibcite{el2018detection}{{24}{}{{}}{{}}}
\bibcite{macris2020all}{{25}{}{{}}{{}}}
\bibcite{mignacco2020role}{{26}{}{{}}{{}}}
\bibcite{belkin2019reconciling}{{27}{}{{}}{{}}}
\bibcite{sen2008collective}{{28}{}{{}}{{}}}
\bibcite{rozemberczki2021multi}{{29}{}{{}}{{}}}
\bibcite{maurya2021improving}{{30}{}{{}}{{}}}
\bibcite{velivckovic2017graph}{{31}{}{{}}{{}}}
\bibcite{wu2019simplifying}{{32}{}{{}}{{}}}
\bibcite{wang2022powerful}{{33}{}{{}}{{}}}
\bibcite{he2021bernnet}{{34}{}{{}}{{}}}
\bibcite{wang1987stochastic}{{35}{}{{}}{{}}}
\bibcite{malliaros2013clustering}{{36}{}{{}}{{}}}
\bibcite{lu2021learning}{{37}{}{{}}{{}}}
\bibcite{baranwal2021graph}{{38}{}{{}}{{}}}
\bibcite{li2022finding}{{39}{}{{}}{{}}}
\bibcite{luan2021heterophily}{{40}{}{{}}{{}}}
\bibcite{zhang2014phase}{{41}{}{{}}{{}}}
\bibcite{mezard1987spin}{{42}{}{{}}{{}}}
\bibcite{talagrand2002gaussian}{{43}{}{{}}{{}}}
\bibcite{carmona2006universality}{{44}{}{{}}{{}}}
\bibcite{panchenko2013sherrington}{{45}{}{{}}{{}}}
\bibcite{sato2020survey}{{46}{}{{}}{{}}}
\bibcite{geerts2022expressiveness}{{47}{}{{}}{{}}}
\bibcite{xu2018how}{{48}{}{{}}{{}}}
\bibcite{gilmer2017neural}{{49}{}{{}}{{}}}
\bibcite{scarselli2018vapnik}{{50}{}{{}}{{}}}
\bibcite{deshpande2017asymptotic}{{51}{}{{}}{{}}}
\bibcite{mossel2018proof}{{52}{}{{}}{{}}}
\bibcite{pennington2017nonlinear}{{53}{}{{}}{{}}}
\bibcite{keriven2022not}{{54}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{LastPage}{{}{11}{}{equation.0.17}{}}
\newlabel{LastPage}{{}{11}{}{page.11}{}}
\xdef\lastpage@lastpage{11}
\xdef\lastpage@lastpageHy{11}
\gdef \@abspage@last{11}
